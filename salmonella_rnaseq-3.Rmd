---
title: 'RNA-Seq Read Processing and Alignment with TopHat2 of Salmonella enteritidis and S. typhimurium knock-outs grown without added Fe'
author: "Emily Giroux"
date: "5/1/2019"
output: pdf_document
fontsize: 11pt
geometry: margin=1in
urlcolor: blue
header-includes: \usepackage{xcolor}
---

```{r, global_options, eval=TRUE, echo=FALSE, cache=TRUE}
#Set the global options for knitr
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=80), tidy = TRUE, fig.align='center',
               cache=FALSE, collapse=TRUE, echo=FALSE, eval=FALSE, include=FALSE,
               message=FALSE, quietly=TRUE, results='hide', warn.conflicts=FALSE, 
               warning=FALSE)
```

```{r, installation1, eval=TRUE, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
#Installing required packages
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)

if(!require(devtools)) install.packages("devtools")
devtools::install_github("kassambara/fastqcr")

if (!requireNamespace("BiocManager"))
    install.packages("BiocManager")
BiocManager::install()

library("BiocManager")
.cran_packages <- c("ggplot2", "gridExtra", "rprojroot", "data.table", 
                    "knitr", "kableExtra", "cowplot", "filesstrings",
                    "tidyr", "reshape2", "kableExtra")
.bioc_packages <- c("ape", "BiocStyle", "Biostrings", "dada2", "edgeR")
.inst <- .cran_packages %in% installed.packages()
if(any(!.inst)) {
   install.packages(.cran_packages[!.inst])
}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)) {
  BiocManager::install(.bioc_packages[!.inst], ask = FALSE)
}
sapply(c(.cran_packages, .bioc_packages), require, character.only = TRUE)
```

```{r sourcing_my_functions, echo=FALSE, eval=TRUE, include=FALSE, cache=TRUE}
#Source our custom R scripts:    
#For this we will use the rprojroot package to set the directory structures. This will help us when finding our files to source functions. We specify ours is an RStudio project. The root object contains a function that will help us locate our package R files regarless of our current working directory.
library(rprojroot)
root <- rprojroot::is_rstudio_project
scriptsPath <- root$make_fix_file(".")("R")
scripts  <- dir(root$find_file("R", path = root$find_file()))
scriptsl <- paste(scriptsPath, scripts, sep = "//")
lapply(scriptsl, source)
# Record the path to the environment images directory:
sharedPath <- "/isilon/cfia-ottawa-fallowfield/users/girouxeml/PIRL_working_directory/"
analysis   <- "salmonella_rnaseq/"
sharedPathAn <- paste(sharedPath, analysis, sep = "")

imageDirPath <- "/isilon/cfia-ottawa-fallowfield/users/girouxeml/GitHub_Repos/r_environments/Salmonella_RNA-Seq/"
baseImage    <- "salmonella_processing_28May2019.RData"

save.image(paste(imageDirPath, baseImage, sep = ""))
load(paste(imageDirPath, baseImage, sep = ""))
```

Many of the commands and approach are from:
Simon Anders et al., 2013. Count-based differential expression analysis of 
RNA sequencing data using R and Bioconductor. Nature protocols | VOL.8 NO.9 | 2013

Chunk: Analysis and Sequence Data Directory Setting
User:
Define the the folder in the shared folder that will hold the analyses of the time-course/dataset 
you will be working with. In our case, we have two different time-course experiments, Oosporogenesis 
and Oospore Conversion. Below we set which one the script will run analyses for. We also get the 
user to specify what the name of the directory that will hold the reads will be.

User:
The following paths are to directories where the references, tools and general 
requirements are located, this depends on the directories actually having been 
put there:
```{r user_tools_and_references_dir, cache=TRUE}
referencesPath   <- paste(sharedPath, "References/", sep = "")
projRefPath      <- paste(referencesPath, "salmonella_transcriptomes/", sep = "") 
# Name of the directory you'll be keeping all the analyses for this pipeline:
workDir    <- "HiSeq_Analyses2"
#Name of the directory to keep the fastq files in:
seqDataDir <- "HiSeq_data2"
```
User:
Specify the name and path of the csv file you would like to use for generation of the 
metadata file:
This file should have all the rad processing information.
```{r metadata_name_and_path, cache=TRUE}
metadataFile <- "HiSeq_Salmonella_NoFe.csv"
metadataPath <- paste(referencesPath, metadataFile, sep = "")
```

Below the metadata file(s) specified by the user is/are read into R:
```{r copy_metadata_to_analysis_dir_and_read, cache=TRUE}
library(data.table)
metadata <-  fread(metadataPath, sep = ",", header = TRUE, quote = "")
```

The following chunk will integrate the user-defined variables from the previous chunk into the script.
```{r creating_dir_for_analysis, cache=TRUE}
# Create fastq directory in sharedPath folder based on "seqDataDir":
dir.create(paste(sharedPathAn, seqDataDir, sep = ""), 
           showWarnings = TRUE, 
           recursive    = FALSE)
# Set the path the fastq directory:
pathFastq <- paste(sharedPathAn, seqDataDir, "/", sep = "")
```
Note on downloading the NCBI nr and nt databases:     
These can be downloaded locally using the update_blast.pl script. Create a separate directory for nt and nr, then when in the directory, run the command, for example, in the nr directory:     
     
$ /opt/bio/ncbi-blast+/bin/update_blast.pl --decompress nr     
      
Only the files that are missing/out-of-date or have a new time stamp on NCBI will be updated. Note the databses are very large, and it takes a very long time to get them all. Consider the same and time requirements.     
      

**Define path variables to programs and scripts used:**
```{r}
# Biocluster system-wide programs:
blastallPath    <- "/opt/bio/ncbi-blast+/bin/blastn"
blastxDBnrPath  <- "/isilon/cfia-ottawa-fallowfield/users/girouxeml/Databases/ncbi/blastdb/reference/nr/nr"
blastxDBntPath  <- "/isilon/cfia-ottawa-fallowfield/users/girouxeml/Databases/ncbi/blastdb/reference/nt/nt"
bowtie2BuildPath <- "/opt/bio/bowtie2/bin/bowtie2-build"
samtools1Path    <- "/opt/bio/samtools1/bin/samtools"
deconSeqPath     <- "/opt/bio/DeconSeq/deconseq.pl"
seqPrepPath      <- "/opt/bio/SeqPrep/SeqPrep"
starPath         <- "/opt/bio/STAR/bin/STAR"
tophat2Path      <- "/opt/bio/tophat/bin/tophat2"

# CFIA-ACIA users home directory programs:
progPath        <- "/home/CFIA-ACIA/girouxeml/prog/"

# Need to install locally using conda:
# htseq-count, prinseq-lite + prinseq-graphs (prinseq), tophat, bowtie2
htseqCountPath <- paste(progPath, "miniconda/envs/htseq/bin/htseq-count", sep = "")
tophatPath <- paste(progPath, "miniconda/envs/htseq/bin/tophat2", sep = "")
blastpPath      <- paste(progPath, "miniconda/bin/blastp", sep = "")
fastqPEValPath <- paste(progPath, "tools/FastqPairedEndValidator.pl", sep = "")
prinSeqPath     <- paste(progPath, "miniconda/envs/prinseq/bin/prinseq-lite.pl", sep = "")
prinSeqGraphPath <- paste(progPath, "miniconda/envs/prinseq/bin/prinseq-graphs-noPCA.pl", sep = "")
# Note - to get prinSeqGraphs to work, I had to modify the version that omits Statistics::PCA, and specify the lib to Cairo and JSON at the top of the executable, replacing the call-outs to use Cairo and use JSON that are in the packaged executable.
# See: https://alvinalexander.com/perl/edu/articles/pl010015/

saltyRefPathOriginalName    <- paste(projRefPath, "saltyp_GCA_000006945.2_ASM694v2_genomic.fna", sep = "")
saltyRefPath    <- paste(projRefPath, "saltyp_ref.fa", sep = "")
saltygff3Path   <- paste(projRefPath, "saltyp_GCA_000006945.2_ASM694v2_genomic.gff", sep = "")
saltyTranscript <- paste(projRefPath, "saltyp_GCA_000006945.2_ASM694v2_cds_from_genomic.fna", sep = "")
saltyProteins   <- paste(projRefPath, "saltyp_GCA_000006945.2_ASM694v2_protein.faa", sep = "")
salenRefPathOriginalName    <- paste(projRefPath, "salent_GCA_000009505.1_ASM950v1_genomic.fna", sep = "")
salenRefPath    <- paste(projRefPath, "salent_ref.fa", sep = "")
salengff3Path   <- paste(projRefPath, "salent_GCA_000009505.1_ASM950v1_genomic.gff", sep = "")
salenTranscript <- paste(projRefPath, "salent_GCA_000009505.1_ASM950v1_cds_from_genomic.fna", sep = "")
salenProteins   <- paste(projRefPath, "salent_GCA_000009505.1_ASM950v1_protein.faa", sep = "")

# Obtain the following by converting the gff to gtf using cufflinks gffread
# /opt/bio/cufflinks/bin/gffread input.gff -T -o output.gtf
saltygtfPath    <- paste(projRefPath, "saltyp_GCA_000006945.2_ASM694v2_genomic.gtf", sep = "")
salengtfPath   <- paste(projRefPath, "salent_GCA_000009505.1_ASM950v1_genomic.gtf", sep = "")
```

**Checking for Adapter Sequences**     
Cutadapt. We are replacing the code for SeqPrep with the use of cutadapt and workflow developed by Benjamin Callahan. See page: https://benjjneb.github.io/dada2/ITS_workflow.html      

Chunk: Library Adapter Sequence User Input   
User needs to specify the adapter sequences attached to the sequencing reads. This will depend on 
how the libraries were prepared.    
```{r}
# Notice that the adapter sequences we chose when processing the HiSeq reads
# is from the same region we are choosing for our MiSeq reads, only shorter:

fwdAdapGQ    <- "AGATCGGAAGAGCACACGTCTGAACTCCAGTCA"  # HiSeq Genome Quebec
revAdapGQ    <- "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT"  # HiSeq Genome Quebec

fwdAdapMiSeq <- "AGATCGGAAGAGCACAC"   # MiSeq
revAdapMiSeq <- "AGATCGGAAGAGCGTCGT"  # MiSeq

fwdAdap      <- fwdAdapGQ
revAdap      <- revAdapGQ
```

Add the basecalls names to the metadata table
```{r}
metadata$BaseCallsName  <- paste(metadata$FilenamePrefix, "_", 
                                 metadata$Read_Direction, ".fastq", sep = "")
```


Start the qsub prefixes from M onwards - A-L was for analyzing the RNA-Seq set one data that included samples grown in media with added iron, while this set focusses only on those samples grown without iron added to growth media.     
      
Copy and gunzip files:
```{r}
prefix <- "M_copy_unzip"
cmd <- with(metadata, paste("cp ", 
                FastqFilePath, " ", 
                #sharedPathAn, seqDataDir, "/", basename(FastqFilePath),"\n",
                pathFastq, FilenamePrefix, "_", Read_Direction, ".fastq.gz", sep = ""))

suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```
Clean-up step:
Remove the output files while keeping the qsub and bash file:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

Make a metadata table called metadataRawPairs that has the raw reads rows 
collapsed by R1 and R2
```{r}
library("reshape2")
metadata$SppAbbr <- gsub("_.*", "", metadata$LibraryName)
metadataRawPairs <- dcast(data = metadata, LibraryName + FilenamePrefix + Experiment 
                          + Condition + TimePoint + SppAbbr
                          + RNASeq_Replicate + RNA_Replicate + ScientificName
                          ~ Read_Direction, value.var = "BaseCallsName", FUN = c)

metadataRawPairs$ExpUnit <- sub("-DIP_.*", "", metadataRawPairs$FilenamePrefix)
metadataRawPairs$FastqPath <- pathFastq
```

Run FastqPairedEndValidator for the first time on the raw read pairs:
*** Note: Fastq.gz need to be uncompressed to run.
```{r}
prefix <- "N_Validator"
cmd <-  with (metadataRawPairs, paste(fastqPEValPath, 
                                      " ", pathFastq, R1, " ", pathFastq, R2, sep = ""))
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```

To show the output of each pair on the console in Rstudio:
```{r}
for (k in 1:nrow(metadataRawPairs)) {
  cat(c(k, metadataRawPairs$R1[k], metadataRawPairs$R2[k]))
  system(paste("cat ", sharedPathAn, prefix, "/", prefix, k, suffix, ".o*", sep = ""))
  cat("\n")
}
```

To remove the output files (only the .e* files) after you are done:
```{r}
system(paste("rm ", sharedPathAn, prefix, "/", prefix, "*", suffix, ".e*", sep = ""))
```

Create the custom `AllOrients' function for primer sequences for all possible orientations. See: https://benjjneb.github.io/dada2/ITS_workflow.html      
```{r, mkAllOrientsFn, echo=TRUE, eval=TRUE, include=TRUE, tidy=FALSE, linewidth=80}
AllOrients   <- function(primer) {
     require(Biostrings)
     dna     <- DNAString(primer)
     orients <- c(Forward    = dna, 
                  Complement = Biostrings::complement(dna), 
                  Reverse    = reverse(dna), 
                  RevComp    = reverseComplement(dna))
    return(sapply(orients, toString))
}
```

We can now use the custom AllOrients function to generate the primer sequences in all possible orientations in which they may be found:
```{r, runAllOrientsFn, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, cache=TRUE, comment=NA}
fwdOrients <- AllOrients(fwdAdap)
revOrients <- AllOrients(revAdap)
fwdOrients
```

Create the custom `PrimerHits' function for checking our sequences for all orientations of primer sequences as generated above using the AllOrients function. This function generates a table of counts of the number of reads in which the primer is found. 
```{r, mkPrimerHitsFn, echo=TRUE, eval=TRUE, include=TRUE, tidy=FALSE, linewidth=80}
PrimerHits <- function(primer, fn) {
    nhits  <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
```

Before checking our sequences for primers with cutadapt, we need to remove those sequences with ambiguous bases. Ambiguous bases (Ns) in the sequencing reads makes accurate mapping of short primer sequences difficult, so we "pre-filter" these sequences so that we only remove those with Ns and perform no other fitlering.
```{r, filterNsfqs, echo=TRUE, eval=FALSE, include=TRUE, tidy=FALSE, linewidth=80}
# Create a directory for the cutadapted fastq files, only if it doesn't already exist. 
filtNsPath <- file.path(pathFastq, "removedNs")
cutAdaptDir <- file.path(pathFastq, "cutadapt")
if(!dir.exists(filtNsPath)) dir.create(filtNsPath)
if(!dir.exists(cutAdaptDir)) dir.create(cutAdaptDir)
cutAdaptDir <- paste(pathFastq, "cutadapt/", sep = "")

# Also create the output filenames for the N-filtered fastq files.
filtF <- file.path(filtNsPath, paste(metadataRawPairs$FilenamePrefix, "_F.fastq.gz", sep = ""))
filtR <- file.path(filtNsPath, paste(metadataRawPairs$FilenamePrefix, "_R.fastq.gz", sep = ""))

# Also create the output filenames for the cutadapt-ed fastq files.
cutFqs <- file.path(cutAdaptDir, paste(metadataRawPairs$FilenamePrefix, "_Fcut.fastq.gz", sep = ""))
cutRqs <- file.path(cutAdaptDir, paste(metadataRawPairs$FilenamePrefix, "_Rcut.fastq.gz", sep = ""))

library(dada2)

# Run DADA2's filterAndTrim function to remove sequences with ambiguous bases:
filterAndTrim(paste(metadataRawPairs$FastqPath, metadataRawPairs$R1, ".gz", sep = ""), 
              filtF, 
              paste(metadataRawPairs$FastqPath, metadataRawPairs$R2, ".gz", sep = ""),
              filtR, 
              maxN = 0, multithread = TRUE, verbose = TRUE)
```
We can now check the N-filtered sequences of just one sample set for primer hits:
```{r, cntPrimerHits, echo=TRUE, eval=TRUE, tidy=FALSE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA}
library("ShortRead")
rbind(FWD.ForwardReads = sapply(fwdOrients, PrimerHits, fn = filtF[1]),
      FWD.ReverseReads = sapply(fwdOrients, PrimerHits, fn = filtR[1]), 
      REV.ForwardReads = sapply(revOrients, PrimerHits, fn = filtF[1]), 
      REV.ReverseReads = sapply(revOrients, PrimerHits, fn = filtR[1]))
```
As Expected, the vast majority of forward primer is found in its forward orientation, and in some of the reverse reads in its reverse-complement orientation (due to read-through when the read is short). Similarly, the reverse primer is found with its expected orientations.      
     
Set up the paths and fastq file input and output names in preparation for running cutadapt. 
\textbf{Note:} You may need to install cutadapt locally if it is not installed system wide. I chose to use conda to install it locally:
```{r, preCutadapt, echo=TRUE, eval=FALSE, include=TRUE, tidy=FALSE, linewidth=80}
cutadapt <- "/home/CFIA-ACIA/girouxeml/prog/miniconda/envs/qiime2/bin/cutadapt"
```

Now we can proceed to using cutadapt to remove primer sequences at the ends of our reads.     
We'll use dada2:::rc()      
     
An aside:  The ::: is one of 2 possible namespace operators in R. This triple-colon operator acts like a double-colon operator (which selects definitions from a particular namespace), AND allows access to hidden objects. In this command, we are specifying that we want to use the `rc` function that is from the dada2 package, not the `rc` function that may also exist in base R or other packages we possibly loaded. See this page for more on namespace operators in R: http://r-pkgs.had.co.nz/namespace.html      
     
In the following, we use dada2:::rc to get the reverse complement of the primer sequences. The idea is that it can be used to compare each sequence and its reverse complement to a reference sequence(s) in the right orientation, and then choose the orientation that minimizes that distance. See page: https://github.com/benjjneb/dada2/issues/451       
      
         
Cutadapt flag parameter explanations:    
    
Parameter | Definition
--------- | -------------------------------------------------------------
**-g:** | Regular 5' adapter/primer sequence on forward reads
**-a:** | Regular 3' adapter/primer sequence on forward reads
**-G:** | Regular 5' adapter/primer sequence on reverse reads
**-A:** | Regular 3' adapter/primer sequence on reverse reads
**-p:** | Specify paired-end sequencing reads
**-o:** | Specify output files
**-n:** | Number of times to trim when more than one adapter is present in a read
     
Documentation for fine-tuning use of cutadapt is available at: https://cutadapt.readthedocs.io/en/v1.7.1/guide.html 
```{r, cutadapt, echo=TRUE, eval=FALSE, include=TRUE, tidy=FALSE, linewidth=80}
fwdPrimerRC <- dada2:::rc(fwdAdap)
revPrimerRC <- dada2:::rc(revAdap)
# Trim fwdPrimer and the reverse-complement of the revPrimer off forward reads:
fwdFlags <- paste("-g", fwdAdap, "-a", revPrimerRC)
# Trim revPrimer and the reverse-complement of the fwdPrimer off reverse reads:
revFlags <- paste("-G", revAdap, "-A", fwdPrimerRC)
# Run Cutadapt
for(i in seq_along(metadataRawPairs$FilenamePrefix)) {
  system2(cutadapt, args = c(fwdFlags, revFlags, "-n", 2,
                             "-o", cutFqs[i], "-p", cutRqs[i],
                             filtF[i], filtR[i]))
}
save.image(paste(imageDirPath, chptImage, sep = ""))
```
\textbf{Note:} There is a lot of output generated to the console when running cutadapt.

As a sanity check, we will count the presence of primers in the first cutadapt-ed sample:
```{r, cntPrimerHits2, echo=-1, eval=TRUE, include=TRUE, tidy=FALSE, linewidth=80, cache=TRUE, comment=NA}
library("ShortRead")
rbind(FWD.ForwardReads = sapply(fwdOrients, PrimerHits, fn = cutFqs[1]),
      FWD.ReverseReads = sapply(fwdOrients, PrimerHits, fn = cutRqs[1]), 
      REV.ForwardReads = sapply(revOrients, PrimerHits, fn = cutFqs[1]), 
      REV.ReverseReads = sapply(revOrients, PrimerHits, fn = cutRqs[1]))
```
We can also use fastqcr to get quality summary statistics for all the fastq files and output the fastqc results reports to a new directory within the directroy containing the raw fastq files:    
```{r, fastqcr1, echo=TRUE, eval=FALSE, tidy=FALSE, linewidth=80, cache=TRUE}
library("fastqcr")
# Set the processed fastq file and reports directories:
cutAdaptQCDir <- file.path(cutAdaptDir, "fastqcRes")
if(!dir.exists(cutAdaptQCDir)) dir.create(cutAdaptQCDir)

threads <- 4

# Run the fastqcr command:
fastqc(fq.dir = cutAdaptDir, qc.dir = cutAdaptQCDir, threads = threads)

# Aggregate the fastqc results for the processed fastq files:
qcCutAdapt   <- qc_aggregate(cutAdaptQCDir, progressbar = TRUE)
```

Alternatively, we can generate an HTML report that will aggregate QC summaries and metrics across all of our ITS samples:     
\textbf{Note:} There have been issues running this reliably, and an alternative is being considered.
```{r, fastqcr5, echo=TRUE, eval=FALSE, tidy=FALSE, linewidth=80, cache=TRUE}
fastqcr::qc_report(qc.path = cutAdaptQCDir, 
                   result.file = paste(cutAdaptDir, 
                                       "multiFastqcProcessed", sep = ""),
                   experiment = "Adapter-removed fastq report",
                   interpret = TRUE, preview = TRUE)
```

*** Note - a significant portion of the reasds do not overlap - and merging and only using merged reads would mean losing a significant amount of data. This is because only 125 bp x 2, 250 cycles of sequencing were performed using the HiSeq. For this reason, the unmerged, processed reads will be used instead of the merged reads.

Record the name of the adapRem fastq files output from processing with SeqPrep:
```{r}
adapRemFastq <- list.files(path = cutAdaptDir, pattern = "cut.fastq.gz")
```

*** Note, for HiSeq I didn't remove the single reads because I am still assessing results of merging 
the reads. Delete unnecessary fastq files - this should be adjusted so that it matches the basename 
we originally brought in as it is the original fastq raw reads we no longer need to keep in our data 
directory.
```{r}
# Specify the pattern that will match the raw read files
deleteFastq <- list.files(path = pathFastq, pattern = ".*fastq.gz$")

for(k in 1:length(deleteFastq)) {
  cmd <- paste("rm ", pathFastq, deleteFastq[k], sep = "")
  system(cmd)
}

# Specify the pattern that will match the raw read files that were N-filtered:
deleteFastq <- list.files(path = filtNsPath, pattern = ".*fastq.gz$")

for(k in 1:length(deleteFastq)) {
  cmd <- paste("rm ", filtNsPath, "/", deleteFastq[k], sep = "")
  system(cmd)
}
```

```{r}
library("reshape2")
metadataRawPairs$R1cut <- paste(metadataRawPairs$FilenamePrefix, "Fcut.fastq", sep = "_")
metadataRawPairs$R2cut <- paste(metadataRawPairs$FilenamePrefix, "Rcut.fastq", sep = "_")
```

```{r}
gunzipFastq <- list.files(path = cutAdaptDir, pattern = "*.fastq.gz$")
for(k in 1:length(gunzipFastq)) {
  cmd <- paste("gunzip ", cutAdaptDir, gunzipFastq[k], sep = "")
  system(cmd)
}
```

Second round of FastqPairedEndValidator with adapters removed
```{r}
prefix <- "O_Validator" 
cmd <-  with(metadataRawPairs, 
             paste(fastqPEValPath, 
                   " ", cutAdaptDir, paste(FilenamePrefix, "_Fcut.fastq", sep = ""), 
                   " ", cutAdaptDir, paste(FilenamePrefix, "_Rcut.fastq", sep = ""), 
                   sep = ""))

suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```

To show the output of each pair on the console in Rstudio
```{r}
for (k in 1:nrow(metadataRawPairs)) {
  cat(c(k, metadataRawPairs$R1cut[k], metadataRawPairs$R2cut[k]))
  system(paste("cat ", sharedPathAn, prefix, "/", prefix, k, suffix, ".o*", sep = ""))
  cat("\n")
}
```

To remove the output files after you are done
```{r}
system(paste("rm ", sharedPathAn, prefix, "/", prefix, "*", suffix, ".e*", sep = ""))
```

Unfortunately PrinSeq requires fastq be decompressed when running in paired-end more. Only when using single reads can files be compressed, using zcat with the input to STDIN for PrinSeq.
```{r}
prefix <- "P_PrinSeqGD"
cmd    <- MakePrinSeqGraphFiles(metadataRawPairs, metadataRawPairs$FilenamePrefix, cutAdaptDir, 
                                paste(metadataRawPairs$FilenamePrefix, "_Fcut.fastq", sep = ""), 
                                prefix, "adapRem", 
                                paste(metadataRawPairs$FilenamePrefix, "_Rcut.fastq", sep = ""))
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```

To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

Add the path to the raw graphs .gd files to the metadataRawPairs tabel:
```{r}
for(k in 1:nrow(metadataRawPairs)){
  metadataRawPairs$AdapRemGraphs <- paste(metadataRawPairs$FilenamePrefix, ".adapRem.gd", sep = "") 
}
```
Load graph (.gd) files on PrinSeq webserver to see quality reports.

Note: There is a problem with PrinSeq and the number of reads going in for processing are almost all removed in the output. Trying Trimmomatic instead.      
1-1. Filter adapRem.fastq output from Cutadapt by quality.
```{r}
trimmoPath <- "/home/CFIA-ACIA/girouxeml/prog/miniconda/envs/trimmomatic/bin/trimmomatic"
node <- 10
prefix <- "Q_trimmomatic"
cmd <- with(metadataRawPairs, 
            paste("conda activate trimmomatic && ", trimmoPath, 
                  " PE -threads ", node,
                  " -validatePairs ",
                  " -trimlog ", paste(sharedPathAn, prefix, "/", FilenamePrefix, ".log", sep = ""),
                  " -summary ", paste(sharedPathAn, prefix, "/", FilenamePrefix, ".stats.log", sep = ""),
                  " ", paste(cutAdaptDir, R1cut, " ", cutAdaptDir, R2cut, sep = ""),
                  " -baseout ", paste(cutAdaptDir, FilenamePrefix, ".fastq", sep = ""),
                  " SLIDINGWINDOW:3:30 ",
                  " MINLEN:60 ",
                  " && conda deactivate ",
                  sep = ""
                  ))
cmd[1]
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```
To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

```{r}
metadataRawPairs$trimR1paired <- paste(metadataRawPairs$FilenamePrefix, "_1P.fastq", sep = "")
metadataRawPairs$trimR2paired <- paste(metadataRawPairs$FilenamePrefix, "_2P.fastq", sep = "")
metadataRawPairs$trimR1single <- paste(metadataRawPairs$FilenamePrefix, "_1U.fastq", sep = "")
metadataRawPairs$trimR2single <- paste(metadataRawPairs$FilenamePrefix, "_2U.fastq", sep = "")
```

After trimming, we need to repair any reads that are not in order, for paired-end reads. Previously I ran TopHat2 without first repairing and there were warnings: "read pairing issues detected!"     
```{r}
repairBBmap <- "/home/CFIA-ACIA/girouxeml/prog/miniconda/envs/trimmomatic/bin/repair.sh"
prefix <- "R_repairPairs"

cmd <- with(metadataRawPairs,
            paste("conda activate trimmomatic && ", repairBBmap,
                  " in1=", paste(cutAdaptDir, trimR1paired, sep = ""),
                  " in2=", paste(cutAdaptDir, trimR2paired, sep = ""),
                  " out=", paste(cutAdaptDir, FilenamePrefix, "_fixed_1P.fastq", sep = ""),
                  " out2=", paste(cutAdaptDir, FilenamePrefix, "_fixed_2P.fastq", sep = ""),
                  " outs=", paste(cutAdaptDir, FilenamePrefix, "_fixed_singles.fastq", sep = ""),
                  " && conda deactivate", sep = ""))
cmd[2]
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```

To remove the output files after you are done:
```{r}
system(paste("rm ", sharedPathAn, prefix, "/", prefix, "*", suffix, ".o*", sep = ""))
```

```{r}
metadataRawPairs$trimFixR1paired <- paste(metadataRawPairs$FilenamePrefix, "_fixed_1P.fastq", sep = "")
metadataRawPairs$trimFixR2paired <- paste(metadataRawPairs$FilenamePrefix, "_fixed_2P.fastq", sep = "")
```

Bowtie:
Creating Bowtie reference index from the fasta file: 
```{r}
bowind <- "saltyp_ref"
cmd    <- paste(bowtie2BuildPath, " -f ", saltyRefPath, " ", paste(projRefPath, bowind, sep = ""), sep = "")
system(cmd)

bowind <- "salent_ref"
cmd    <- paste(bowtie2BuildPath, " -f ", salenRefPath, " ", paste(projRefPath, bowind, sep = ""), sep = "")
system(cmd)
```


TopHat:   
Using TopHat2, even though tis is for prokaryote. TopHat2 is a splice-aware mapper, and used for eukaryotic analyses which have exons. 
Prokaryotes don't have this, but can still use TopHat2. Easier since my script is here already. However, another tool called HISAT2 
may be much faster - consider this for the future.   
Create folders to put Tophat results and runs the jobs.  
```{r}
# For paired-end not merged:
for(j in 1:length(metadataRawPairs$FilenamePrefix)) {
  dir.create(paste(sharedPathAn, metadataRawPairs$FilenamePrefix[j], sep = ""),
             showWarnings = TRUE, recursive = FALSE)
}

metadataRawPairs$refGFFPath <- metadataRawPairs$SppAbbr
metadataRawPairs$refGFFPath <- sub("Sent", salengff3Path, metadataRawPairs$refGFFPath)
metadataRawPairs$refGFFPath <- sub("Styp", saltygff3Path, metadataRawPairs$refGFFPath)

metadataRawPairs$refGTFPath <- metadataRawPairs$SppAbbr
metadataRawPairs$refGTFPath <- sub("Sent", salengtfPath, metadataRawPairs$refGTFPath)
metadataRawPairs$refGTFPath <- sub("Styp", saltygtfPath, metadataRawPairs$refGTFPath)

metadataRawPairs$bowind <- metadataRawPairs$SppAbbr
metadataRawPairs$bowind <- sub("Sent", "salent_ref", metadataRawPairs$bowind)
metadataRawPairs$bowind <- sub("Styp", "saltyp_ref", metadataRawPairs$bowind)
```

Only way to fix name pairing issues is to omit using the single unpaired reads.:
```{r}
prefix <- "S_TophatQsub"
node   <- 2
cmd <- with(metadataRawPairs, 
            paste("conda activate htseq && ", tophatPath, 
                  " -G ", refGTFPath,
                  " -p ", node, 
                  " -o ", sharedPathAn, FilenamePrefix, "/", FilenamePrefix,".TopHat.",
                          format(Sys.time(), "%Y-%m-%d"),
                  " ",    projRefPath, bowind,
                  " ",    cutAdaptDir, trimFixR1paired, #doesn't work with singles: ",", cutAdaptDir, trimFixR2paired,
                  " ",    cutAdaptDir, trimFixR2paired, #doesn't work with singles: ",", cutAdaptDir, trimR2single,
                  " && conda deactivate", sep = ""))
cmd[2]
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix, node)
```
To remove the output files after you are done:
```{r}
system(paste("rm ", sharedPathAn, prefix, "/", prefix, "*", suffix, ".o*", sep = ""))
```

Setting up the folder date
```{r}
# could have automatic search for most recent
topHatDate <- ".2019-05-24"
```

MUST DO CLEAN UP OF TMP FILES WITHIN FOLDERS
- fix the matching to temp tophat files to be removed
```{r}
# For paired-end not merged
cmd <- with(metadataRawPairs, paste("rm -r ", sharedPathAn, metadataRawPairs$FilenamePrefix, "/", 
                                   metadataRawPairs$FilenamePrefix, ".TopHat", topHatDate, "/tmp", sep = ""))
system(cmd)
```

To run Samtools on the Tophat folder that has the right date
```{r}
prefix <- "T_SamtoolsSortQsub"
cmd <- with(metadataRawPairs,  # For paired-end not merged
            (paste(samtools1Path, " sort",   " -n ", # sort bam files by name
                   paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix, ".TopHat", 
                         topHatDate, "/", "accepted_hits.bam ", sep = ""), " -o ",
                   paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix, ".TopHat", 
                         topHatDate, "/", FilenamePrefix, "_sn.bam", sep = ""),
                   " && ",
                   samtools1Path, " view ", " -o ", # convert bam files to sam for htseq-count
                   paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix, ".TopHat", 
                         topHatDate, "/", FilenamePrefix, "_sn.sam ", sep = ""),
                   paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix, ".TopHat", 
                         topHatDate, "/", FilenamePrefix, "_sn.bam", sep = ""),
                   " && ",
                   samtools1Path, " sort ", # for IGV - sort bam files by position
                   paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix, ".TopHat", 
                         topHatDate, "/", "accepted_hits.bam ", sep = ""), " -o ", 
                   paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix, ".TopHat", 
                         topHatDate, "/", FilenamePrefix, "_s.bam", sep = ""),
                   " && ",
                   samtools1Path, " index ", # for IGV - index sorted bam files for IGV
                   paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix, ".TopHat",
                         topHatDate, "/", FilenamePrefix, "_s.bam", sep = ""),
                   sep = "")))
node   <- 1
cmd[1]
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix, node)
```
To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

HTSeq-count for TopHat2 hits:  
Prepare the metadata table for recording TopHat counts:
```{r}
# For paired-end not merged
metadataRawPairs$countTopHat <- paste(metadataRawPairs$FilenamePrefix, "TopHat2Count_genes_locusTag", sep = ".")

metadataRawPairs$countPath <- paste(sharedPathAn, metadataRawPairs$FilenamePrefix, 
                                    "/", metadataRawPairs$FilenamePrefix, ".TopHat", topHatDate, "/",
                                    metadataRawPairs$countTopHat, sep ="")
```

Add a column to metadataRawPairs that transforms the RNASeq replicates and RNA replicates into a single value representing "Replicate"
```{r}
metadataRawPairs$Replicate <- paste(metadataRawPairs$RNASeq_Replicate, metadataRawPairs$RNA_Replicate, sep = "_")
unique(metadataRawPairs$Replicate)
metadataRawPairs$Replicate <- sub("^1_1", "1", metadataRawPairs$Replicate)
metadataRawPairs$Replicate <- sub("^2_1", "2", metadataRawPairs$Replicate)
metadataRawPairs$Replicate <- sub("^2_2", "3", metadataRawPairs$Replicate)
metadataRawPairs$Replicate <- sub("^2_3", "4", metadataRawPairs$Replicate)
```

Add a column to specifiy if library is experimental (knock-outs) versus wild-type:
```{r}
metadataRawPairs$Treatment <- metadataRawPairs$Condition
metadataRawPairs$Treatment <- sub("^ko342", "ko", metadataRawPairs$Treatment)
metadataRawPairs$Treatment <- sub("^ko42", "ko", metadataRawPairs$Treatment)
metadataRawPairs$Treatment <- sub("^ko8", "ko", metadataRawPairs$Treatment)
```

To complete the Processing Stage, write the metadata table to future record and for when 
continuing on to data analysis:
```{r}
# Specify which metadata table you've been using:
finalMetadata <- metadataRawPairs
finalName <- "Salmonella_27May2019_metadataTable_afterProcessing.csv"
write.table(finalMetadata, file = paste(sharedPathAn, finalName, sep = ""), 
            append = FALSE, quote = FALSE, 
            sep = ",", row.names = FALSE)
```


Read count with HT-Seq:   
Great majority of reads classified as 'no_feature'. This is becuase it's only taking type=exon, and the only exon types in the gtf are for RNA, so only the RNA are counted. To capture counts for RNA and CDS, I changed all CDS in the gtf files to exon, and used the idattr transcript_id, since genes and RNA have this. Because bacteria don't have introns, all the cds are exons anyway, and there will not be multiple exons per cds - it's a 1:1 ratio. I want to count RNA so that I can see the amount ribosomal in the reads. The resulting count tables make a lot more sense. Also - include the --additional-attr=gene_name so that an extra column with the gene names is included in the out put - this saves us having to match up transcript_ids and locus_tag names later on.
```{r}
# Set HT-Seq options:
stranded <- "no"
MINAQUAL <- 10
prefix   <- "U_HTSeq_Qsub-GeneName"
node     <- 1

cmd <- with(metadataRawPairs,  # For paired-end not merged
            paste("conda activate htseq && ", htseqCountPath, 
                  " -s ", stranded,
                  " -a ", MINAQUAL,
                  #" --idattr=transcript_id ", # removed --type=CDS # I replaced all feature names CDS with EXON (bacteria) # used transcript_id instead of gene_id so that rna were counted.
                  #" --additional-attr=gene_name ",
                  " --additional-attr=locus_tag ",
                  " --idattr=ID ",
                  " --type=gene ", # If not specified, default is exon
                  " ", paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix,".TopHat", 
                        topHatDate, "/", FilenamePrefix, "_sn.sam ", sep = ""),
                  # refGTFExonsPath, " > ",
                  refGFFPath, " > ",
                  paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix,".TopHat", 
                        topHatDate, "/", metadataRawPairs$countTopHat, sep = ""),  # PE not merged
                        " && conda deactivate", sep = ""))
cmd[1]
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```
To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

Edit the counts tables:     
1. Keep only the Locus_tag and count columns     
2. Remove rows of duplicated locus_tags. to remove rows where there are no counts for a locus_tag - this will also remove duplicate locus_tags since every locus_tag duplication had one of the duplicated locus_tags that have a 0 count.     
3. Remove the non-informative count summaries at the end of the files.   
Now we have count tables that use the locus_tags instead of the unhelpful default gene IDs, no rRNAs, and no extra summery counts at the end to remove.
```{r}
for(i in 1:length(metadataRawPairs$countPath)){
  tbl <- read.table(metadataRawPairs$countPath[i], sep = "\t", header = FALSE, colClasses = c(V1="NULL"))
  tbl2 <- tbl[is.finite(rowSums(log(tbl[-1]))),]
  tbl2[tbl2==""] <- NA
  tbl2 <- na.omit(tbl2)
  write.table(tbl2, file = paste(sharedPathAn, metadataRawPairs$FilenamePrefix[i], "/", 
                                 metadataRawPairs$FilenamePrefix[i], ".TopHat", topHatDate, "/",
                                 metadataRawPairs$countTopHat[i], "_rowsWnoneSkipped", sep = ""),
              append = FALSE, sep = "\t", row.names = FALSE, na = )
  }
```

Add the path tot he edited count tables to the metadata table:
```{r}
metadataRawPairs$finalCountsPath <- paste(sharedPathAn, metadataRawPairs$FilenamePrefix, "/", 
                                          metadataRawPairs$FilenamePrefix, ".TopHat", topHatDate, "/",
                                          metadataRawPairs$countTopHat, "_rowsWnoneSkipped", sep = "")
```

Make 2 separate tables, one for Salent, and the other for Saltyp - Critical - because the gene names/locus tags are different.
```{r}
library(data.table)
dt <- as.data.table(metadataRawPairs)
setkey(dt, SppAbbr)
salentMetaDt <- dt["Sent"]
saltypMetaDt <- dt["Styp"]

# Identify the count files and read them into R using readDGE:
library("edgeR")
# Turn the count of the list into a data.frame
countsSent <- readDGE(salentMetaDt$finalCountsPath)$counts
countsStyp <- readDGE(saltypMetaDt$finalCountsPath)$counts

# Simplify the column names to remove full path and keep only FilenamePrefix:
colnames(countsSent) <- basename(colnames(countsSent))
colnames(countsStyp) <- basename(colnames(countsStyp))

write.table(countsSent, file = file.path(sharedPathAn, "sent_counts_GenesOnly_readDGE_TopHat.annotated.csv"),
            sep = ",", row.names = TRUE, col.names = NA, quote = FALSE) 

write.table(countsStyp, file = file.path(sharedPathAn, "styp_counts_GenesOnly_readDGE_TopHat.annotated.csv"),
            sep = ",", row.names = TRUE, col.names = NA, quote = FALSE) 
```

Filter weakly-expressed features without at least 1 read per million in the n of the samples, where n is the size of the smallest group of replicates (here, n = 1 for the single knockdown - 8 - group)
```{r}
library("Biostrings")
library("edgeR")
library("reshape2")

cpmsSent <- cpm(countsSent)  # counts per million
cpmsStyp <- cpm(countsStyp)  # counts per million

keep <- rowSums(cpmsSent >1) >= 1
countsSent  <- countsSent[keep,]
keep <- rowSums(cpmsStyp >1) >= 1
countsStyp  <- countsStyp[keep,]

# Visualize and inspect the count table:
head(countsSent[,order(salentMetaDt$Condition)], 5)
head(countsStyp[,order(saltypMetaDt$Condition)], 5)
```

```{r}
# Create a DGEList object (edgeR's container for RNA-seq count data):
dSent <- DGEList(counts = countsSent, group = salentMetaDt$Condition)
dStyp <- DGEList(counts = countsStyp, group = saltypMetaDt$Condition)

# Estimate normalization factors using:
dSent <- calcNormFactors(dSent)
dStyp <- calcNormFactors(dStyp)
```

Inspect the relationships between samples using a multidimensional scaling (MDS) plot:
```{r}
# For S. enteriditis:
plotMDS(dSent, labels = salentMetaDt$Condition, 
        col = rainbow(length(levels(factor(salentMetaDt$Condition))))[factor(salentMetaDt$Condition)], 
        cex = 1, main = "Salmonella enteritidis MDS") 
```

```{r}
# For S. typhimurium:
plotMDS(dStyp, labels = saltypMetaDt$Condition, 
        col = rainbow(length(levels(factor(saltypMetaDt$Condition))))[factor(saltypMetaDt$Condition)], 
        cex = 1, main = "Salmonella typhimurium MDS") 
```

Print the MDS files to a fresh results directory:
```{r}
# Create the results directories for each species:
resultsSent <- "sent_28-May-2019-TopHat-edgeR"
dir.create(paste(sharedPathAn, resultsSent, sep = ""), showWarnings = TRUE, recursive = FALSE)

resultsStyp <- "styp_28-May-2019-TopHat-edgeR"
dir.create(paste(sharedPathAn, resultsStyp, sep = ""), showWarnings = TRUE, recursive = FALSE)

# Generate the MDS plots:
pdf(file.path(paste(sharedPathAn, resultsSent, sep = ""), "sent_MDS-edgeR_TopHat.pdf")) 
plotMDS(dSent, labels = salentMetaDt$Condition, 
        col = rainbow(length(levels(factor(salentMetaDt$Condition))))[factor(salentMetaDt$Condition)], 
        cex = 1, main = "Salmonella enteritidis MDS") 
dev.off()

pdf(file.path(paste(sharedPathAn, resultsStyp, sep = ""), "styp_MDS-edgeR_TopHat.pdf"))
plotMDS(dStyp, labels = saltypMetaDt$Condition, 
        col = rainbow(length(levels(factor(saltypMetaDt$Condition))))[factor(saltypMetaDt$Condition)], 
        cex = 1, main = "Salmonella typhimurium MDS") 
dev.off()
```

Estimate tagwise dispersion (simple design):
```{r}
dSent <- estimateCommonDisp(dSent)
dSent <- estimateTagwiseDisp(dSent)
dStyp <- estimateCommonDisp(dStyp)
dStyp <- estimateTagwiseDisp(dStyp)
```

Create a visual representation of the mean-variance relationship using the plotMeanVar and plotBCV function:
```{r}
# For S. enteriditis
plotMeanVar(dSent, show.tagwise.vars = TRUE, NBline = TRUE)
plotBCV(dSent)
```

```{r}
# For S. typhimurium
plotMeanVar(dStyp, show.tagwise.vars = TRUE, NBline = TRUE)
plotBCV(dStyp)
```
Print to file the plotMeanVar and BCV plots:
```{r}
# For S. enteriditis
pdf(file.path(paste(sharedPathAn, resultsSent, sep = ""), "sent_plotMeanVar-edgeR.pdf")) 
plotMeanVar(dSent, show.tagwise.vars = TRUE, NBline = TRUE)
dev.off()
pdf(file.path(paste(sharedPathAn, resultsSent, sep = ""), "sent_plotBCV-edgeR.pdf")) 
plotBCV(dSent)
dev.off()

# For S. typhimurium
pdf(file.path(paste(sharedPathAn, resultsStyp, sep = ""), "styp_plotMeanVar-edgeR.pdf")) 
plotMeanVar(dStyp, show.tagwise.vars = TRUE, NBline = TRUE)
dev.off()
pdf(file.path(paste(sharedPathAn, resultsStyp, sep = ""), "styp_plotBCV-edgeR.pdf")) 
plotBCV(dStyp)
dev.off()
```
Test for differential expression for complex designs - Here our designs are the same for both species, so no need to create two separate design matrices:
```{r}
library(edgeR)
design <- model.matrix(object = ~Condition, salentMetaDt)
design
```
Estimate pairwise dispersion values, relative to the design matrix, using the Cox-Reid (CR)-adjusted likelihood:
```{r}
# For S. enteriditis:
d2Sent <- estimateGLMTrendedDisp(dSent, design)
d2Sent <- estimateGLMTagwiseDisp(d2Sent, design)

# For S. typhimurium:
d2Styp <- estimateGLMTrendedDisp(dStyp, design)
d2Styp <- estimateGLMTagwiseDisp(d2Styp, design)
```

Given the design matrix and dispersion estimates, fit a GLM to each feature:
```{r}
fSent <- glmFit(d2Sent, design)
fStyp <- glmFit(d2Styp, design)
```

Perform a likelihood ratio test specifying the difference of interest (here, knockouts vesus control, which corresponds to the WHICH column in the design matrix)
```{r}
deSent <- glmLRT(fSent, coef = 2:4)
deStyp <- glmLRT(fStyp, coef = 2:4)
```

Use the topTags function to present a tabular summary of the differential expression statistics:
```{r}
ttSent <- topTags(deSent, n=nrow(dSent))
head(ttSent$table)

ttStyp <- topTags(deStyp, n=nrow(dStyp))
head(ttStyp$table)
```
Inspect the depth-adjusted reads per million for some of the top differentially expressed genes:
```{r}
ncSent <- cpm(dSent, normalized.lib.sizes = TRUE)
ncStyp <- cpm(dStyp, normalized.lib.sizes = TRUE)

rnSent <- rownames(ttSent$table)
rnStyp <- rownames(ttStyp$table)

head(ncSent[rnSent, order(salentMetaDt$Condition)],5)
head(ncStyp[rnStyp, order(saltypMetaDt$Condition)],5)
```
Create a graphical summary, such as an M (log-fold change) versus A (log-average expression) plot, here showing the genes selected as differentially expressed (with a 5% false discovery rate):
```{r}
degSent <- rnSent[ttSent$table$FDR < .05]
degStyp <- rnStyp[ttStyp$table$FDR < .05]

plotSmear(dSent, de.tags = degSent)
plotSmear(dStyp, de.tags = degStyp)
```
Save the results tables:
```{r}
write.csv(ttSent$table, file = paste(sharedPathAn, resultsSent, "/sent_topTags_edgeR.csv", sep = ""))
write.csv(ttStyp$table, file = paste(sharedPathAn, resultsStyp, "/styp_topTags_edgeR.csv", sep = ""))
```

     
# Below chunks are not runable yet.      
# The chunks below need editing to work - they rely on fixing an issue with the gene lengths. 
Ideas for the following to extract genelength for names being locus_tags: https://raw.githubusercontent.com/tgirke/GEN242/gh-pages/_vignettes/09_Rsequences/Rsequences.Rmd
```{r}
library(Biostrings)
sentFa <- readDNAStringSet(salenRefPath)
sentGff <- import(salengff3Path)
sentGffGene <- sentGff[values(sentGff)[,"type"]=="gene"]
sentGene <- DNAStringSet(Views(sentFa[[1]], IRanges(start(sentGffGene), end(sentGffGene))))
names(sentGene) <- values(sentGffGene)[,"locus_tag"]

stypFa <- readDNAStringSet(saltyRefPath)
stypGff <- import(saltygff3Path)
stypGffGene <- stypGff[values(stypGff)[,"type"]=="gene"]
stypGene <- DNAStringSet(Views(stypFa[[1]], IRanges(start(stypGffGene), end(stypGffGene))))
names(stypGene) <- values(stypGffGene)[,"locus_tag"]
```

```{r}
geneLengthSent <- setNames(width(sentGene), names(sentGene))
tmpLenSent <- data.table(geneLengthSent)
tmpLenSent$gene <- names(geneLengthSent)
colnames(tmpLenSent)[colnames(tmpLenSent)=="geneLengthSent"] <- "width"
tmpLenSent <- na.omit(tmpLenSent) # remove rows with no width or no locus tag
length(tmpLenSent$gene)
# [1] 4206 # This is the number for genes-width pairs in the table
length(unique(tmpLenSent$gene))
# [1] 4200 # This is the number of unique genes in the table - there are 6 duplicated ones

# Note - we can add a column for gene length to the counts table, BUT there are a few genes that have 2 gene length entries for the same locus tag - these may be tRNAs that are made of 2 fragments. The total fragment size is not calculated, so it's not predictable which length for which fragment gets added to the table in the commands below. I'll keep these commands, but not use the table generated until this can be fixed.
tempSent <- data.frame(countsSent)
colnames(tempSent) <- colnames(countsSent)
tempSent$gene <- rownames(tempSent)
tempSent$width <- rownames(tempSent)
tempSent <- as.data.table(tempSent)

# Match gene width to gene name/locus_tag in the coutns table:
setkey(tempSent, width)
setkey(tmpLenSent, gene)
tempSent[tmpLenSent, width := width]
tempSent$gene <- rownames(tempSent)
tempSent$Row.names <- NULL
tempSent$y <- geneLengthSent

tempSent2 <- setdiff(tmpLenSent, tempSent$gene) # See note above on fragmented tRNAs that occupy 2 ros or more in the genelengths table.


# Need to do the same to add gene lengths for saltyp set. 


# Until issue with gene length is solved (above), can't do rpkm normalized to gene length here.
# rpkm edgeR Salent rna-seq:
summary(temp2sent)
library("edgeR")
salent_rpkmCount <- rpkm(temp2sent[,1:4], # All columns except y which is col 5
                         gene.length = temp2sent$y, normalized.lib.sizes = TRUE)
salent_rpkmCount[1:5,]

# rpkm edgeR Salty rna-seq:
summary(temp2styp)
library("edgeR")
saltyp_rpkmCount <- rpkm(temp2styp[,1:4], # All columns except y which is col 5
                         gene.length = temp2styp$y, normalized.lib.sizes = TRUE)
saltyp_rpkmCount[1:5,]
```

# Note - below chunk uses normalization based on rpkm AND cpm - whcih we can't do until the genelength duplicate issue is solved (previous chunk)
Setting up for Differential Analysis: 
Counts per million (cpm) from the gene counts from TopHat: 
```{r}
library("reshape2")
# For both Sent and Styp:
metadataColsToMerge <- c("LibraryName", "FilenamePrefix", "Condition", "Replicate", "ExpUnit") 
condition <- c("ko342", "ko42", "ko8", "control")

# For Sent:
countsRawSent <- salent_rpkmCount[,1:4] # instead of temp2Sent
cpmsSent <- cpm(countsRawSent) # counts per million
countsForBarPlotsSent <- melt(cpmsSent, id.vars = colnames(cpmsSent))
colnames(countsForBarPlotsSent)[1] <- "SequenceID"
colnames(countsForBarPlotsSent)[2] <- "ExpUnit"
colnames(countsForBarPlotsSent)[3] <- "Read_Counts"
countsSent <- merge(countsForBarPlotsSent, unique(metadataRawPairs[,c(metadataColsToMerge)]),  
                    by = "ExpUnit", all.x = TRUE)
countsSent$Condition <- factor(countsSent$Condition, levels = c(condition))
length(unique(countsSent$Condition))
countsSent[1:5,]

# For Styp:
countsRawStyp <- saltyp_rpkmCount[,1:4] # instead of temp2Styp
cpmsStyp <- cpm(countsRawStyp) # counts per million
countsForBarPlotsStyp <- melt(cpmsStyp, id.vars = colnames(cpmsStyp))
colnames(countsForBarPlotsStyp)[1] <- "SequenceID"
colnames(countsForBarPlotsStyp)[2] <- "ExpUnit"
colnames(countsForBarPlotsStyp)[3] <- "Read_Counts"
countsStyp <- merge(countsForBarPlotsStyp, unique(metadataRawPairs[,c(metadataColsToMerge)]),  
                    by = "ExpUnit", all.x = TRUE)
countsStyp$Condition <- factor(countsStyp$Condition, levels = c(condition))
length(unique(countsStyp$Condition))
countsStyp[1:5,]
```
# To do following, need to fix gene length issue as well.
Finding potential house-keeping (HK) genes:
```{r}
# For Sent:
colnames(salent_rpkmCount)
logRpkmCountSent      <- data.frame(log10(salent_rpkmCount[,1:4]+1))
logRpkmCountSent$mean <- apply(logRpkmCountSent, 1, mean)
logRpkmCountSent$min  <- apply(logRpkmCountSent, 1, min)
logRpkmCountSent$max  <- apply(logRpkmCountSent, 1, max)
logRpkmCountSent$SD   <- apply(logRpkmCountSent, 1, sd)

potentialHKSent <- subset(logRpkmCountSent, min > 0 & SD < 0.25)
potentialHKSent <- potentialHKSent[order(-potentialHKSent$mean),]  

write.table(potentialHKSent, 
            file   = file.path(paste(sharedPathAn, sep = ""), "sent_potential_houseKeepingGenes.csv"),
            append = FALSE, sep = ",", col.names = NA)

# For Styp:
colnames(saltyp_rpkmCount)
logRpkmCountStyp      <- data.frame(log10(saltyp_rpkmCount[,1:4]+1))
logRpkmCountStyp$mean <- apply(logRpkmCountStyp, 1, mean)
logRpkmCountStyp$min  <- apply(logRpkmCountStyp, 1, min)
logRpkmCountStyp$max  <- apply(logRpkmCountStyp, 1, max)
logRpkmCountStyp$SD   <- apply(logRpkmCountStyp, 1, sd)

potentialHKStyp <- subset(logRpkmCountStyp, min > 0 & SD < 0.25)
potentialHKStyp <- potentialHKStyp[order(-potentialHKStyp$mean),]  

write.table(potentialHKStyp, 
            file   = file.path(paste(sharedPathAn, sep = ""), "styp_potential_houseKeepingGenes.csv"),
            append = FALSE, sep = ",", col.names = NA)
```