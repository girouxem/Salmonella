---
title: 'RNA-Seq Read Processing and Alignment with TopHat2 of Salmonella enteritidis and S. typhimurium knock-outs grown without added Fe'
author: "Emily Giroux"
date: "5/1/2019"
output: pdf_document
fontsize: 11pt
geometry: margin=1in
urlcolor: blue
header-includes: \usepackage{xcolor}
---

```{r, global_options, eval=TRUE, echo=FALSE, cache=TRUE}
#Set the global options for knitr
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=80), tidy = TRUE, fig.align='center',
               cache=FALSE, collapse=TRUE, echo=FALSE, eval=FALSE, include=FALSE,
               message=FALSE, quietly=TRUE, results='hide', warn.conflicts=FALSE, 
               warning=FALSE)
```

```{r, installation1, eval=TRUE, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
#Installing required packages
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)

if(!require(devtools)) install.packages("devtools")
devtools::install_github("kassambara/fastqcr")

if (!requireNamespace("BiocManager"))
    install.packages("BiocManager")
BiocManager::install()

library("BiocManager")
.cran_packages <- c("ggplot2", "gridExtra", "rprojroot", "data.table", 
                    "knitr", "kableExtra", "cowplot", "filesstrings",
                    "tidyr", "reshape2", "kableExtra")
.bioc_packages <- c("ape", "BiocStyle", "Biostrings", "dada2", "edgeR")
.inst <- .cran_packages %in% installed.packages()
if(any(!.inst)) {
   install.packages(.cran_packages[!.inst])
}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)) {
  BiocManager::install(.bioc_packages[!.inst], ask = FALSE)
}
sapply(c(.cran_packages, .bioc_packages), require, character.only = TRUE)
```

```{r sourcing_my_functions, echo=FALSE, eval=TRUE, include=FALSE, cache=TRUE}
#Source our custom R scripts:    
#For this we will use the rprojroot package to set the directory structures. This will help us when finding our files to source functions. We specify ours is an RStudio project. The root object contains a function that will help us locate our package R files regarless of our current working directory.
library(rprojroot)
root <- rprojroot::is_rstudio_project
scriptsPath <- root$make_fix_file(".")("R")
scripts  <- dir(root$find_file("R", path = root$find_file()))
scriptsl <- paste(scriptsPath, scripts, sep = "//")
lapply(scriptsl, source)
# Record the path to the environment images directory:
sharedPath <- "/isilon/cfia-ottawa-fallowfield/users/girouxeml/PIRL_working_directory/"
analysis   <- "salmonella_rnaseq/"
sharedPathAn <- paste(sharedPath, analysis, sep = "")

imageDirPath <- paste(sharedPathAn, "salmonellaRmd_data/", sep = "")
baseImage    <- "salmonella_processing_28May2019.RData"

save.image(paste(imageDirPath, baseImage, sep = ""))
load(paste(imageDirPath, baseImage, sep = ""))
```

Many of the commands and approach are from:
Simon Anders et al., 2013. Count-based differential expression analysis of 
RNA sequencing data using R and Bioconductor. Nature protocols | VOL.8 NO.9 | 2013

Chunk: Analysis and Sequence Data Directory Setting
User:
Define the the folder in the shared folder that will hold the analyses of the time-course/dataset 
you will be working with. In our case, we have two different time-course experiments, Oosporogenesis 
and Oospore Conversion. Below we set which one the script will run analyses for. We also get the 
user to specify what the name of the directory that will hold the reads will be.

User:
The following paths are to directories where the references, tools and general 
requirements are located, this depends on the directories actually having been 
put there:
```{r user_tools_and_references_dir, cache=TRUE}
referencesPath   <- paste(sharedPath, "References/", sep = "")
projRefPath      <- paste(referencesPath, "salmonella_transcriptomes/", sep = "") 
# Name of the directory you'll be keeping all the analyses for this pipeline:
workDir    <- "HiSeq_Analyses2"
#Name of the directory to keep the fastq files in:
seqDataDir <- "HiSeq_data2"
```
User:
Specify the name and path of the csv file you would like to use for generation of the 
metadata file:
This file should have all the rad processing information.
```{r metadata_name_and_path, cache=TRUE}
metadataFile <- "HiSeq_Salmonella_NoFe.csv"
metadataPath <- paste(referencesPath, metadataFile, sep = "")
```

Below the metadata file(s) specified by the user is/are read into R:
```{r copy_metadata_to_analysis_dir_and_read, cache=TRUE}
library(data.table)
metadata <-  fread(metadataPath, sep = ",", header = TRUE, quote = "")
```

The following chunk will integrate the user-defined variables from the previous chunk into the script.
```{r creating_dir_for_analysis, cache=TRUE}
# Create fastq directory in sharedPath folder based on "seqDataDir":
dir.create(paste(sharedPathAn, seqDataDir, sep = ""), 
           showWarnings = TRUE, 
           recursive    = FALSE)
# Set the path the fastq directory:
pathFastq <- paste(sharedPathAn, seqDataDir, "/", sep = "")
```
Note on downloading the NCBI nr and nt databases:     
These can be downloaded locally using the update_blast.pl script. Create a separate directory for nt and nr, then when in the directory, run the command, for example, in the nr directory:     
     
$ /opt/bio/ncbi-blast+/bin/update_blast.pl --decompress nr     
      
Only the files that are missing/out-of-date or have a new time stamp on NCBI will be updated. Note the databses are very large, and it takes a very long time to get them all. Consider the same and time requirements.     
      

**Define path variables to programs and scripts used:**
```{r}
# Biocluster system-wide programs:
blastallPath    <- "/opt/bio/ncbi-blast+/bin/blastn"
blastxDBnrPath  <- "/isilon/cfia-ottawa-fallowfield/users/girouxeml/Databases/ncbi/blastdb/reference/nr/nr"
blastxDBntPath  <- "/isilon/cfia-ottawa-fallowfield/users/girouxeml/Databases/ncbi/blastdb/reference/nt/nt"
bowtie2BuildPath <- "/opt/bio/bowtie2/bin/bowtie2-build"
samtools1Path    <- "/opt/bio/samtools1/bin/samtools"
deconSeqPath     <- "/opt/bio/DeconSeq/deconseq.pl"
seqPrepPath      <- "/opt/bio/SeqPrep/SeqPrep"
starPath         <- "/opt/bio/STAR/bin/STAR"
tophat2Path      <- "/opt/bio/tophat/bin/tophat2"

# CFIA-ACIA users home directory programs:
progPath        <- "/home/CFIA-ACIA/girouxeml/prog/"

# Need to install locally using conda:
# htseq-count, prinseq-lite + prinseq-graphs (prinseq), tophat, bowtie2
htseqCountPath <- paste(progPath, "miniconda/envs/htseq/bin/htseq-count", sep = "")
tophatPath <- paste(progPath, "miniconda/envs/htseq/bin/tophat2", sep = "")
blastpPath      <- paste(progPath, "miniconda/bin/blastp", sep = "")
fastqPEValPath <- paste(progPath, "tools/FastqPairedEndValidator.pl", sep = "")
prinSeqPath     <- paste(progPath, "miniconda/envs/prinseq/bin/prinseq-lite.pl", sep = "")
prinSeqGraphPath <- paste(progPath, "miniconda/envs/prinseq/bin/prinseq-graphs-noPCA.pl", sep = "")
# Note - to get prinSeqGraphs to work, I had to modify the version that omits Statistics::PCA, and specify the lib to Cairo and JSON at the top of the executable, replacing the call-outs to use Cairo and use JSON that are in the packaged executable.
# See: https://alvinalexander.com/perl/edu/articles/pl010015/

#condaPath <- paste(progPath, "miniconda/condabin/conda", sep = "")
#cmd <- paste("conda -h ")
#system(cmd)
# system2(condaPath)

saltyRefPathOriginalName    <- paste(projRefPath, "saltyp_GCA_000006945.2_ASM694v2_genomic.fna", sep = "")
saltyRefPath    <- paste(projRefPath, "saltyp_ref.fa", sep = "")
saltygff3Path   <- paste(projRefPath, "saltyp_GCA_000006945.2_ASM694v2_genomic.gff", sep = "")
saltyTranscript <- paste(projRefPath, "saltyp_GCA_000006945.2_ASM694v2_cds_from_genomic.fna", sep = "")
saltyProteins   <- paste(projRefPath, "saltyp_GCA_000006945.2_ASM694v2_protein.faa", sep = "")
salenRefPathOriginalName    <- paste(projRefPath, "salent_GCA_000009505.1_ASM950v1_genomic.fna", sep = "")
salenRefPath    <- paste(projRefPath, "salent_ref.fa", sep = "")
salengff3Path   <- paste(projRefPath, "salent_GCA_000009505.1_ASM950v1_genomic.gff", sep = "")
salenTranscript <- paste(projRefPath, "salent_GCA_000009505.1_ASM950v1_cds_from_genomic.fna", sep = "")
salenProteins   <- paste(projRefPath, "salent_GCA_000009505.1_ASM950v1_protein.faa", sep = "")

# Obtain the following by converting the gff to gtf using cufflinks gffread
# /opt/bio/cufflinks/bin/gffread input.gff -T -o output.gtf
saltygtfPath    <- paste(projRefPath, "saltyp_GCA_000006945.2_ASM694v2_genomic.gtf", sep = "")
salengtfPath   <- paste(projRefPath, "salent_GCA_000009505.1_ASM950v1_genomic.gtf", sep = "")

# Obtain the following by replacing all CDS features to exon features in the gtf files (after you copy them)
# sed -i 's/CDS/exon/g' saltyp_GCA_000006945.2_ASM694v2_genomic.allExon.gtf
saltygtfExonPath    <- paste(projRefPath, "saltyp_GCA_000006945.2_ASM694v2_genomic.allExon.gtf", sep = "")
salengtfExonPath   <- paste(projRefPath, "salent_GCA_000009505.1_ASM950v1_genomic.allExon.gtf", sep = "")
```

**Checking for Adapter Sequences**     
Cutadapt. We are replacing the code for SeqPrep with the use of cutadapt and workflow developed by Benjamin Callahan. See page: https://benjjneb.github.io/dada2/ITS_workflow.html      

Chunk: Library Adapter Sequence User Input   
User needs to specify the adapter sequences attached to the sequencing reads. This will depend on 
how the libraries were prepared.    
```{r}
# Notice that the adapter sequences we chose when processing the HiSeq reads
# is from the same region we are choosing for our MiSeq reads, only shorter:

fwdAdapGQ    <- "AGATCGGAAGAGCACACGTCTGAACTCCAGTCA"  # HiSeq Genome Quebec
revAdapGQ    <- "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT"  # HiSeq Genome Quebec

fwdAdapMiSeq <- "AGATCGGAAGAGCACAC"   # MiSeq
revAdapMiSeq <- "AGATCGGAAGAGCGTCGT"  # MiSeq

fwdAdap      <- fwdAdapGQ
revAdap      <- revAdapGQ
```

Add the basecalls names to the metadata table
```{r}
metadata$BaseCallsName  <- paste(metadata$FilenamePrefix, "_", 
                                 metadata$Read_Direction, ".fastq", sep = "")
```


Start the qsub prefixes from M onwards - A-L was for analyzing the RNA-Seq set one data that included samples grown in media with added iron, while this set focusses only on those samples grown without iron added to growth media.     
      
Copy and gunzip files:
```{r}
prefix <- "M_copy_unzip"
cmd <- with(metadata, paste("cp ", 
                FastqFilePath, " ", 
                #sharedPathAn, seqDataDir, "/", basename(FastqFilePath),"\n",
                pathFastq, FilenamePrefix, "_", Read_Direction, ".fastq.gz", sep = ""))

suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```
Clean-up step:
Remove the output files while keeping the qsub and bash file:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

Make a metadata table called metadataRawPairs that has the raw reads rows 
collapsed by R1 and R2
```{r}
library("reshape2")
metadata$SppAbbr <- gsub("_.*", "", metadata$LibraryName)
metadataRawPairs <- dcast(data = metadata, LibraryName + FilenamePrefix + Experiment 
                          + Condition + TimePoint + SppAbbr
                          + RNASeq_Replicate + RNA_Replicate + ScientificName
                          ~ Read_Direction, value.var = "BaseCallsName", FUN = c)

metadataRawPairs$ShortName <- paste(metadataRawPairs$SppAbbr,
                                    metadataRawPairs$Experiment, 
                                    metadataRawPairs$Condition, sep = ".")

metadataRawPairs$ExpUnit <- sub("-DIP_.*", "", metadataRawPairs$FilenamePrefix)
metadataRawPairs$FastqPath <- pathFastq
```

Run FastqPairedEndValidator for the first time on the raw read pairs:
*** Note: Fastq.gz need to be uncompressed to run.
```{r}
prefix <- "N_Validator"

cmd <-  with (metadataRawPairs, paste(fastqPEValPath, 
                                      " ", pathFastq, R1, " ", pathFastq, R2, sep = ""))

suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```

To show the output of each pair on the console in Rstudio:
```{r}
for (k in 1:nrow(metadataRawPairs)) {
  cat(c(k, metadataRawPairs$R1[k], metadataRawPairs$R2[k]))
  system(paste("cat ", sharedPathAn, prefix, "/", prefix, k, suffix, ".o*", sep = ""))
  cat("\n")
}
```

To remove the output files (only the .e* files) after you are done:
```{r}
system(paste("rm ", sharedPathAn, prefix, "/", prefix, "*", suffix, ".e*", sep = ""))
```

Create the custom `AllOrients' function for primer sequences for all possible orientations. See: https://benjjneb.github.io/dada2/ITS_workflow.html      
```{r, mkAllOrientsFn, echo=TRUE, eval=TRUE, include=TRUE, tidy=FALSE, linewidth=80}
AllOrients   <- function(primer) {
     require(Biostrings)
     dna     <- DNAString(primer)
     orients <- c(Forward    = dna, 
                  Complement = Biostrings::complement(dna), 
                  Reverse    = reverse(dna), 
                  RevComp    = reverseComplement(dna))
    return(sapply(orients, toString))
}
```

We can now use the custom AllOrients function to generate the primer sequences in all possible orientations in which they may be found:
```{r, runAllOrientsFn, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, cache=TRUE, comment=NA}
fwdOrients <- AllOrients(fwdAdap)
revOrients <- AllOrients(revAdap)
fwdOrients
```

Create the custom `PrimerHits' function for checking our sequences for all orientations of primer sequences as generated above using the AllOrients function. This function generates a table of counts of the number of reads in which the primer is found. 
```{r, mkPrimerHitsFn, echo=TRUE, eval=TRUE, include=TRUE, tidy=FALSE, linewidth=80}
PrimerHits <- function(primer, fn) {
    nhits  <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
```

Before checking our sequences for primers with cutadapt, we need to remove those sequences with ambiguous bases. Ambiguous bases (Ns) in the sequencing reads makes accurate mapping of short primer sequences difficult, so we "pre-filter" these sequences so that we only remove those with Ns and perform no other fitlering.
```{r, filterNsfqs, echo=TRUE, eval=FALSE, include=TRUE, tidy=FALSE, linewidth=80}
# Create a directory for the cutadapted fastq files, only if it doesn't already exist. 
filtNsPath <- file.path(pathFastq, "removedNs")
cutAdaptDir <- file.path(pathFastq, "cutadapt")
if(!dir.exists(filtNsPath)) dir.create(filtNsPath)
if(!dir.exists(cutAdaptDir)) dir.create(cutAdaptDir)
cutAdaptDir <- paste(pathFastq, "cutadapt/", sep = "")

# Also create the output filenames for the N-filtered fastq files.
filtF <- file.path(filtNsPath, paste(metadataRawPairs$FilenamePrefix, "_F.fastq.gz", sep = ""))
filtR <- file.path(filtNsPath, paste(metadataRawPairs$FilenamePrefix, "_R.fastq.gz", sep = ""))

# Also create the output filenames for the cutadapt-ed fastq files.
cutFqs <- file.path(cutAdaptDir, paste(metadataRawPairs$FilenamePrefix, "_Fcut.fastq.gz", sep = ""))
cutRqs <- file.path(cutAdaptDir, paste(metadataRawPairs$FilenamePrefix, "_Rcut.fastq.gz", sep = ""))

library(dada2)

# Run DADA2's filterAndTrim function to remove sequences with ambiguous bases:
filterAndTrim(paste(metadataRawPairs$FastqPath, metadataRawPairs$R1, ".gz", sep = ""), 
              filtF, 
              paste(metadataRawPairs$FastqPath, metadataRawPairs$R2, ".gz", sep = ""),
              filtR, 
              maxN = 0, multithread = TRUE, verbose = TRUE)
```
We can now check the N-filtered sequences of just one sample set for primer hits:
```{r, cntPrimerHits, echo=TRUE, eval=TRUE, tidy=FALSE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA}
library("ShortRead")
rbind(FWD.ForwardReads = sapply(fwdOrients, PrimerHits, fn = filtF[1]),
      FWD.ReverseReads = sapply(fwdOrients, PrimerHits, fn = filtR[1]), 
      REV.ForwardReads = sapply(revOrients, PrimerHits, fn = filtF[1]), 
      REV.ReverseReads = sapply(revOrients, PrimerHits, fn = filtR[1]))
```
As Expected, the vast majority of forward primer is found in its forward orientation, and in some of the reverse reads in its reverse-complement orientation (due to read-through when the read is short). Similarly, the reverse primer is found with its expected orientations.      
     
Set up the paths and fastq file input and output names in preparation for running cutadapt. 
\textbf{Note:} You may need to install cutadapt locally if it is not installed system wide. I chose to use conda to install it locally:
```{r, preCutadapt, echo=TRUE, eval=FALSE, include=TRUE, tidy=FALSE, linewidth=80}
cutadapt <- "/home/CFIA-ACIA/girouxeml/prog/miniconda/envs/qiime2/bin/cutadapt"
```

Now we can proceed to using cutadapt to remove primer sequences at the ends of our reads.     
We'll use dada2:::rc()      
     
An aside:  The ::: is one of 2 possible namespace operators in R. This triple-colon operator acts like a double-colon operator (which selects definitions from a particular namespace), AND allows access to hidden objects. In this command, we are specifying that we want to use the `rc` function that is from the dada2 package, not the `rc` function that may also exist in base R or other packages we possibly loaded. See this page for more on namespace operators in R: http://r-pkgs.had.co.nz/namespace.html      
     
In the following, we use dada2:::rc to get the reverse complement of the primer sequences. The idea is that it can be used to compare each sequence and its reverse complement to a reference sequence(s) in the right orientation, and then choose the orientation that minimizes that distance. See page: https://github.com/benjjneb/dada2/issues/451       
      
         
Cutadapt flag parameter explanations:    
    
Parameter | Definition
--------- | -------------------------------------------------------------
**-g:** | Regular 5' adapter/primer sequence on forward reads
**-a:** | Regular 3' adapter/primer sequence on forward reads
**-G:** | Regular 5' adapter/primer sequence on reverse reads
**-A:** | Regular 3' adapter/primer sequence on reverse reads
**-p:** | Specify paired-end sequencing reads
**-o:** | Specify output files
**-n:** | Number of times to trim when more than one adapter is present in a read
     
Documentation for fine-tuning use of cutadapt is available at: https://cutadapt.readthedocs.io/en/v1.7.1/guide.html 
```{r, cutadapt, echo=TRUE, eval=FALSE, include=TRUE, tidy=FALSE, linewidth=80}
fwdPrimerRC <- dada2:::rc(fwdAdap)
revPrimerRC <- dada2:::rc(revAdap)
# Trim fwdPrimer and the reverse-complement of the revPrimer off forward reads:
fwdFlags <- paste("-g", fwdAdap, "-a", revPrimerRC)
# Trim revPrimer and the reverse-complement of the fwdPrimer off reverse reads:
revFlags <- paste("-G", revAdap, "-A", fwdPrimerRC)
# Run Cutadapt
for(i in seq_along(metadataRawPairs$FilenamePrefix)) {
  system2(cutadapt, args = c(fwdFlags, revFlags, "-n", 2,
                             "-o", cutFqs[i], "-p", cutRqs[i],
                             filtF[i], filtR[i]))
}
save.image(paste(imageDirPath, chptImage, sep = ""))
```
\textbf{Note:} There is a lot of output generated to the console when running cutadapt.

As a sanity check, we will count the presence of primers in the first cutadapt-ed sample:
```{r, cntPrimerHits2, echo=-1, eval=TRUE, include=TRUE, tidy=FALSE, linewidth=80, cache=TRUE, comment=NA}
library("ShortRead")
rbind(FWD.ForwardReads = sapply(fwdOrients, PrimerHits, fn = cutFqs[1]),
      FWD.ReverseReads = sapply(fwdOrients, PrimerHits, fn = cutRqs[1]), 
      REV.ForwardReads = sapply(revOrients, PrimerHits, fn = cutFqs[1]), 
      REV.ReverseReads = sapply(revOrients, PrimerHits, fn = cutRqs[1]))
```
We can also use fastqcr to get quality summary statistics for all the fastq files and output the fastqc results reports to a new directory within the directroy containing the raw fastq files:    
```{r, fastqcr1, echo=TRUE, eval=FALSE, tidy=FALSE, linewidth=80, cache=TRUE}
library("fastqcr")
# Set the processed fastq file and reports directories:
cutAdaptQCDir <- file.path(cutAdaptDir, "fastqcRes")
if(!dir.exists(cutAdaptQCDir)) dir.create(cutAdaptQCDir)

threads <- 4

# Run the fastqcr command:
fastqc(fq.dir = cutAdaptDir, qc.dir = cutAdaptQCDir, threads = threads)

# Aggregate the fastqc results for the processed fastq files:
qcCutAdapt   <- qc_aggregate(cutAdaptQCDir, progressbar = TRUE)
```

Alternatively, we can generate an HTML report that will aggregate QC summaries and metrics across all of our ITS samples:     
\textbf{Note:} There have been issues running this reliably, and an alternative is being considered.
```{r, fastqcr5, echo=TRUE, eval=FALSE, tidy=FALSE, linewidth=80, cache=TRUE}
fastqcr::qc_report(qc.path = cutAdaptQCDir, 
                   result.file = paste(cutAdaptDir, 
                                       "multiFastqcProcessed", sep = ""),
                   experiment = "Adapter-removed fastq report",
                   interpret = TRUE, preview = TRUE)
```

*** Note - a significan portion of the reasds do not overlap - and merging and only using merged reads would mean losing a significant amount of data. This is because only 125 bp x 2, 250 cycles of sequencing were performed using the HiSeq. For this reason, the unmerged, processed reads will be used instead of the merged reads.

Record the name of the adapRem fastq files output from processing with SeqPrep:
```{r}
adapRemFastq <- list.files(path = cutAdaptDir, pattern = "cut.fastq.gz")
```

User to specify the time-points, if any, for the RNA-Seq experiment:
```{r}
timePoints <- c("0")
```

PrinSeq Options Setting:
```{r}
nmax           <- "-ns_max_n 1"
trimLeft       <- "-trim_left 0"
trimRight      <- "-trim_right 10"
trimTailLeft   <- "-trim_tail_left 5"
trimTailRight  <- "-trim_tail_right 5"
trimQualWindow <- "-trim_qual_window 3"
trimQualType   <- "-trim_qual_type mean"
trimQualRight  <- "-trim_qual_right 30" #consider 30 after running with Andre's modified gff.
trimQualLeft   <- "-trim_qual_left 30"
trimQualRule   <- "-trim_qual_rule lt"
lcMethod       <- "-lc_method dust"
lcThreshold    <- "-lc_threshold 7"
#outGood        <- "processed_merged" Define by user
outBad         <- " null"
minLen         <- "-min_len 60"
```

*** Note, for HiSeq I didn't remove the single reads because I am still assessing results of merging 
the reads. Delete unnecessary fastq files - this should be adjusted so that it matches the basename 
we originally brought in as it is the original fastq raw reads we no longer need to keep in our data 
directory.
```{r}
# Specify the pattern that will match the raw read files
deleteFastq <- list.files(path = pathFastq, pattern = ".*fastq.gz$")

for(k in 1:length(deleteFastq)) {
  cmd <- paste("rm ", pathFastq, deleteFastq[k], sep = "")
  system(cmd)
}

# Specify the pattern that will match the raw read files that were N-filtered:
deleteFastq <- list.files(path = filtNsPath, pattern = ".*fastq.gz$")

for(k in 1:length(deleteFastq)) {
  cmd <- paste("rm ", filtNsPath, "/", deleteFastq[k], sep = "")
  system(cmd)
}
```

```{r}
library("reshape2")

metadataRawPairs$R1cut <- paste(metadataRawPairs$FilenamePrefix, "Fcut.fastq", sep = "_")
metadataRawPairs$R2cut <- paste(metadataRawPairs$FilenamePrefix, "Rcut.fastq", sep = "_")
```

```{r}
gunzipFastq <- list.files(path = cutAdaptDir, pattern = "*.fastq.gz$")

for(k in 1:length(gunzipFastq)) {
  cmd <- paste("gunzip ", cutAdaptDir, gunzipFastq[k], sep = "")
  system(cmd)
}
```

Second round of FastqPairedEndValidator with adapters removed
```{r}
prefix <- "O_Validator" 
cmd <-  with(metadataRawPairs, 
             paste(fastqPEValPath, 
                   " ", cutAdaptDir, paste(FilenamePrefix, "_Fcut.fastq", sep = ""), 
                   " ", cutAdaptDir, paste(FilenamePrefix, "_Rcut.fastq", sep = ""), 
                   sep = ""))

suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```

To show the output of each pair on the console in Rstudio
```{r}
for (k in 1:nrow(metadataRawPairs)) {
  cat(c(k, metadataRawPairs$R1cut[k], metadataRawPairs$R2cut[k]))
  system(paste("cat ", sharedPathAn, prefix, "/", prefix, k, suffix, ".o*", sep = ""))
  cat("\n")
}
```

To remove the output files after you are done
```{r}
system(paste("rm ", sharedPathAn, prefix, "/", prefix, "*", suffix, ".e*", sep = ""))
```

Unfortunately PrinSeq requires fastq be decompressed when running in paired-end more. Only when using single reads can files be compressed, using zcat with the input to STDIN for PrinSeq.
```{r}
prefix <- "P_PrinSeqGD"
cmd    <- MakePrinSeqGraphFiles(metadataRawPairs, metadataRawPairs$FilenamePrefix, cutAdaptDir, 
                                paste(metadataRawPairs$FilenamePrefix, "_Fcut.fastq", sep = ""), 
                                prefix, "adapRem", 
                                paste(metadataRawPairs$FilenamePrefix, "_Rcut.fastq", sep = ""))
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```

To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

Add the path to the raw graphs .gd files to the metadataRawPairs tabel:
```{r}
for(k in 1:nrow(metadataRawPairs)){
  metadataRawPairs$AdapRemGraphs <- paste(metadataRawPairs$FilenamePrefix, ".adapRem.gd", sep = "") 
}
```

Step 2: html file generation:     
     
Note: Generation of HTML, or any PNG, fails due to problem not using Cairo:ImageSurface 'create' object, and can't succeed in installing library properly - see edited perl script: ~/prog/miniconda/envs/prinseq/bin/prinseq-graphs-noPCA.pl for script with edits and attempts to fix.     
     
Other option is to load them on PrinSeq web to get htmls from graph files.
```{r}
prefix2 <- "P2_PrinSeq_rawHtmlSub"
cmd <- MakePrinSeqHTML(metadataRawPairs, prefix, metadataRawPairs$AdapRemGraphs)

suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix2, suffix)
```

Note: There is a problem with PrinSeq and the number of reads going in for processing are almost all removed in the output. Trying Trimmomatic instead.      
     
Pre-Processing of Merged Reads using PrinSeq::
For the pre-processing with PrinSeq we have three steps, with three sets of qsubs each:
1. Processing input adapter-removed reads with the PrinSeq trim and filtering options
2. Generating graph files of the processed reads
3. Generating html files using the graph files to visualize the outputs
For each set of qsubs, the .log, .gd, and .html outputs are sent to the first folder that also has 
the first stage qsub and bash files. The processed reads are output to the pathfastq folder.
PrinSeq Processing of merged reads:
First stage of quality pre-processing with PrinSeq:

*** Note: There is a problem with script - look at cutadapted fastq and see that there are a lot of sequences, but PrinSeq reports so few remaining. i.e., 3128-42-DIP_1_1 indicates 13912123 sequences, but PrinSeq report says that there are only 5 sequences! I checked this using the command: awk '{s++}END{print s/4}' file.fastq    
     
1-1. Filter adapRem.fastq output from Cutadapt by quality.
```{r}
trimmoPath <- "/home/CFIA-ACIA/girouxeml/prog/miniconda/envs/trimmomatic/bin/trimmomatic"
node <- 10
prefix <- "Q_trimmomatic"
cmd <- with(metadataRawPairs, 
            paste("conda activate trimmomatic && ", trimmoPath, 
                  " PE -threads ", node,
                  " -validatePairs ",
                  " -trimlog ", paste(sharedPathAn, prefix, "/", FilenamePrefix, ".log", sep = ""),
                  " -summary ", paste(sharedPathAn, prefix, "/", FilenamePrefix, ".stats.log", sep = ""),
                  " ", paste(cutAdaptDir, R1cut, " ", cutAdaptDir, R2cut, sep = ""),
                  " -baseout ", paste(cutAdaptDir, FilenamePrefix, ".fastq", sep = ""),
                  " SLIDINGWINDOW:3:30 ",
                  " MINLEN:60 ",
                  " && conda deactivate ",
                  sep = ""
                  ))
cmd[1]
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```
To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

```{r}
metadataRawPairs$trimR1paired <- paste(metadataRawPairs$FilenamePrefix, "_1P.fastq", sep = "")
metadataRawPairs$trimR2paired <- paste(metadataRawPairs$FilenamePrefix, "_2P.fastq", sep = "")
metadataRawPairs$trimR1single <- paste(metadataRawPairs$FilenamePrefix, "_1U.fastq", sep = "")
metadataRawPairs$trimR2single <- paste(metadataRawPairs$FilenamePrefix, "_2U.fastq", sep = "")
```

After trimming, we need to repair any reads that are not in order, for paired-end reads. Previously I ran TopHat2 without first repairing and there were warnings: "read pairing issues detected!"     
```{r}
repairBBmap <- "/home/CFIA-ACIA/girouxeml/prog/miniconda/envs/trimmomatic/bin/repair.sh"
prefix <- "R_repairPairs"

cmd <- with(metadataRawPairs,
            paste("conda activate trimmomatic && ", repairBBmap,
                  " in1=", paste(cutAdaptDir, trimR1paired, sep = ""),
                  " in2=", paste(cutAdaptDir, trimR2paired, sep = ""),
                  " out=", paste(cutAdaptDir, FilenamePrefix, "_fixed_1P.fastq", sep = ""),
                  " out2=", paste(cutAdaptDir, FilenamePrefix, "_fixed_2P.fastq", sep = ""),
                  " outs=", paste(cutAdaptDir, FilenamePrefix, "_fixed_singles.fastq", sep = ""),
                  " && conda deactivate", sep = ""))
cmd[2]
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```

To remove the output files after you are done:
```{r}
system(paste("rm ", sharedPathAn, prefix, "/", prefix, "*", suffix, ".o*", sep = ""))
```

```{r}
metadataRawPairs$trimFixR1paired <- paste(metadataRawPairs$FilenamePrefix, "_fixed_1P.fastq", sep = "")
metadataRawPairs$trimFixR2paired <- paste(metadataRawPairs$FilenamePrefix, "_fixed_2P.fastq", sep = "")
```

Bowtie:
Creating Bowtie reference index from the fasta file: 
```{r}
bowind <- "saltyp_ref"
cmd    <- paste(bowtie2BuildPath, " -f ", saltyRefPath, " ", paste(projRefPath, bowind, sep = ""), sep = "")
system(cmd)

bowind <- "salent_ref"
cmd    <- paste(bowtie2BuildPath, " -f ", salenRefPath, " ", paste(projRefPath, bowind, sep = ""), sep = "")
system(cmd)
```


TopHat:   
Using TopHat2, even though tis is for prokaryote. TopHat2 is a splice-aware mapper, and used for eukaryotic analyses which have exons. 
Prokaryotes don't have this, but can still use TopHat2. Easier since my script is here already. However, another tool called HISAT2 
may be much faster - consider this for the future.   
Create folders to put Tophat results and runs the jobs.  
```{r}
# For paired-end not merged:
for(j in 1:length(metadataRawPairs$FilenamePrefix)) {
  dir.create(paste(sharedPathAn, metadataRawPairs$FilenamePrefix[j], sep = ""),
             showWarnings = TRUE, recursive = FALSE)
}

metadataRawPairs$refGFFPath <- metadataRawPairs$SppAbbr
metadataRawPairs$refGFFPath <- sub("Sent", salengff3Path, metadataRawPairs$refGFFPath)
metadataRawPairs$refGFFPath <- sub("Styp", saltygff3Path, metadataRawPairs$refGFFPath)

metadataRawPairs$refGTFPath <- metadataRawPairs$SppAbbr
metadataRawPairs$refGTFPath <- sub("Sent", salengtfPath, metadataRawPairs$refGTFPath)
metadataRawPairs$refGTFPath <- sub("Styp", saltygtfPath, metadataRawPairs$refGTFPath)

metadataRawPairs$refGTFExonsPath <- metadataRawPairs$SppAbbr
metadataRawPairs$refGTFExonsPath <- sub("Sent", salengtfExonPath, metadataRawPairs$refGTFExonsPath)
metadataRawPairs$refGTFExonsPath <- sub("Styp", saltygtfExonPath, metadataRawPairs$refGTFExonsPath)

metadataRawPairs$bowind <- metadataRawPairs$SppAbbr
metadataRawPairs$bowind <- sub("Sent", "salent_ref", metadataRawPairs$bowind)
metadataRawPairs$bowind <- sub("Styp", "saltyp_ref", metadataRawPairs$bowind)
```

Only way to fix name pairing issues is to omit using the single unpaired reads:    
Except for 1, 5, 9, 10, 17, 22 that worked without error, I kept getting TopHat2 errors about "Read pairing issues detected" and htseq errors about " claims to have an aligned mate which could not be found in an adjacent line". FastqPairedEndValidator.pl detected no errors, BBMap's repair.sh changed nothing, also tried picard-tools FixMateInformation, still did not resolve error. Pairs correctly named when using gedit to look at fastq headers. Changed fastq to fasta using fastx tool. Tried changing tophat command format from:      
      
From: pairedFwd,pairedRev<space>unpairedFwd,unpairedRev     
To: pairedFwd,unpairedFwd <space> pairedRev,unpairedRev     
     
Still no luck. Only thing that worked was to run command with forward and reverse reads whose pairs still existed, and omit the single, unpaired reads:
```{r}
prefix <- "S_TophatQsub"
node   <- 2
cmd <- with(metadataRawPairs, 
            paste("conda activate htseq && ", tophatPath, 
                  " -G ", refGTFPath,
                  " -p ", node, 
                  " -o ", sharedPathAn, FilenamePrefix, "/", FilenamePrefix,".TopHat.",
                          format(Sys.time(), "%Y-%m-%d"),
                  " ",    projRefPath, bowind,
                  " ",    cutAdaptDir, trimFixR1paired, #doesn't work with singles: ",", cutAdaptDir, trimFixR2paired,
                  " ",    cutAdaptDir, trimFixR2paired, #doesn't work with singles: ",", cutAdaptDir, trimR2single,
                  " && conda deactivate", sep = ""))
cmd[2]
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix, node)
```
To remove the output files after you are done:
```{r}
system(paste("rm ", sharedPathAn, prefix, "/", prefix, "*", suffix, ".o*", sep = ""))
```

Setting up the folder date
```{r}
# could have automatic search for most recent
topHatDate <- ".2019-05-24"
```

MUST DO CLEAN UP OF TMP FILES WITHIN FOLDERS
- fix the matching to temp tophat files to be removed
```{r}
# For paired-end not merged
cmd <- with(metadataRawPairs, paste("rm -r ", sharedPathAn, metadataRawPairs$FilenamePrefix, "/", 
                                   metadataRawPairs$FilenamePrefix, ".TopHat", topHatDate, "/tmp", sep = ""))
system(cmd)
```

To run Samtools on the Tophat folder that has the right date
```{r}
prefix <- "T_SamtoolsSortQsub"
cmd <- with(metadataRawPairs,  # For paired-end not merged
            (paste(samtools1Path, " sort",   " -n ", # sort bam files by name
                   paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix, ".TopHat", 
                         topHatDate, "/", "accepted_hits.bam ", sep = ""), " -o ",
                   paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix, ".TopHat", 
                         topHatDate, "/", FilenamePrefix, "_sn.bam", sep = ""),
                   " && ",
                   samtools1Path, " view ", " -o ", # convert bam files to sam for htseq-count
                   paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix, ".TopHat", 
                         topHatDate, "/", FilenamePrefix, "_sn.sam ", sep = ""),
                   paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix, ".TopHat", 
                         topHatDate, "/", FilenamePrefix, "_sn.bam", sep = ""),
                   " && ",
                   samtools1Path, " sort ", # for IGV - sort bam files by position
                   paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix, ".TopHat", 
                         topHatDate, "/", "accepted_hits.bam ", sep = ""), " -o ", 
                   paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix, ".TopHat", 
                         topHatDate, "/", FilenamePrefix, "_s.bam", sep = ""),
                   " && ",
                   samtools1Path, " index ", # for IGV - index sorted bam files for IGV
                   paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix, ".TopHat",
                         topHatDate, "/", FilenamePrefix, "_s.bam", sep = ""),
                   sep = "")))
node   <- 1
cmd[1]
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix, node)
```
To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

HTSeq-count for TopHat2 hits:  
Prepare the metadata table for recording TopHat counts:
```{r}
# For paired-end not merged
metadataRawPairs$countTopHat <- paste(metadataRawPairs$FilenamePrefix, "TopHat2Count", sep = ".")
```

Add a column to metadataRawPairs that transforms the RNASeq replicates and RNA replicates into a single value representing "Replicate"
```{r}
metadataRawPairs$Replicate <- paste(metadataRawPairs$RNASeq_Replicate, metadataRawPairs$RNA_Replicate, sep = "_")
unique(metadataRawPairs$Replicate)
metadataRawPairs$Replicate <- sub("^1_1", "1", metadataRawPairs$Replicate)
metadataRawPairs$Replicate <- sub("^2_1", "2", metadataRawPairs$Replicate)
metadataRawPairs$Replicate <- sub("^2_2", "3", metadataRawPairs$Replicate)
metadataRawPairs$Replicate <- sub("^2_3", "4", metadataRawPairs$Replicate)
```

Add a column to specifiy if library is experimental (knock-outs) versus wild-type:
```{r}
metadataRawPairs$Treatment <- metadataRawPairs$Condition
metadataRawPairs$Treatment <- sub("^ko342", "ko", metadataRawPairs$Treatment)
metadataRawPairs$Treatment <- sub("^ko42", "ko", metadataRawPairs$Treatment)
metadataRawPairs$Treatment <- sub("^ko8", "ko", metadataRawPairs$Treatment)
```

To complete the Processing Stage, write the metadata table to future record and for when 
continuing on to data analysis:
```{r}
# Specify which metadata table you've been using:
finalMetadata <- metadataRawPairs
finalName <- "Salmonella_27May2019_metadataTable_afterProcessing.csv"
write.table(finalMetadata, file = paste(sharedPathAn, finalName, sep = ""), 
            append = FALSE, quote = FALSE, 
            sep = ",", row.names = FALSE)
```


Read count with HT-Seq:   
Great majority of reads classified as 'no_feature'. This is becuase it's only taking type=exon, and the only exon types in the gtf are for RNA, so only the RNA are counted. To capture counts for RNA and CDS, I changed all CDS in the gtf files to exon, and used the idattr transcript_id, since genes and RNA have this. Because bacteria don't have introns, all the cds are exons anyway, and there will not be multiple exons per cds - it's a 1:1 ratio. I want to count RNA so that I can see the amount ribosomal in the reads. The resulting count tables make a lot more sense.
```{r}
# Set HT-Seq options:
stranded <- "no"
MINAQUAL <- 10
prefix   <- "U_HTSeq_Qsub"
node     <- 1

cmd <- with(metadataRawPairs,  # For paired-end not merged
            paste("conda activate htseq && ", htseqCountPath, 
                  " -s ", stranded,
                  " -a ", MINAQUAL,
                  " --idattr=transcript_id ", # removed --type=CDS # I replaced all feature names CDS with EXON (bacteria) # used transcript_id instead of gene_id so that rna were counted.
                  " ", paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix,".TopHat", 
                        topHatDate, "/", FilenamePrefix, "_sn.sam ", sep = ""),
                  refGTFExonsPath, " > ",
                  paste(sharedPathAn, FilenamePrefix, "/", FilenamePrefix,".TopHat", 
                        topHatDate, "/", metadataRawPairs$countTopHat, sep = ""),  # PE not merged
                        " && conda deactivate", sep = ""))
cmd[1]
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```
To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```
# Obtain two tables, one for each species, with first column the gene id, the second column for locus_tag - will need to use this later.
To retrieve the id and locus_tad for each gene entry, run the following perl command in a terminal, then open with Word, copy to excel,
filter for those that start with gene, then remove duplicates based on the column that has the gene ids, then copy to gedit and save as txt file.     
     
perl -nle '($id)=/ID=([^;]*)/; ($prod)=/locus_tag=([^;]*)/; print "$id\t$prod"' selent.gff > salent.ids.txt     
     
     
*** Note make 2 separate tables, one for Salent, and the other for Saltyp - Critical - because the gene names are different for each of these.
```{r}
library(data.table)
dt <- as.data.table(metadataRawPairs)
setkey(dt, SppAbbr)
salentMetaDt <- dt["Sent"]
saltypMetaDt <- dt["Styp"]

# Identify the count files and read them into R using readDGE:
library("edgeR")
# Turn the count of the list into a data.frame
countsSent <- readDGE(paste(sharedPathAn, salentMetaDt$FilenamePrefix, 
                        "/", salentMetaDt$FilenamePrefix, ".TopHat", topHatDate, "/",
                        salentMetaDt$countTopHat, sep =""))$counts

countsStyp <- readDGE(paste(sharedPathAn, saltypMetaDt$FilenamePrefix, 
                        "/", saltypMetaDt$FilenamePrefix, ".TopHat", topHatDate, "/",
                        saltypMetaDt$countTopHat, sep =""))$counts
# Simplify the column names to remove full path and keep only FilenamePrefix:
colnames(countsSent) <- basename(colnames(countsSent))
colnames(countsStyp) <- basename(colnames(countsStyp))

write.table(countsSent, file = file.path(sharedPathAn, "sent_counts_readDGE_TopHat.annotated.csv"),
            sep = ",", row.names = TRUE, col.names = NA, quote = FALSE) 

write.table(countsStyp, file = file.path(sharedPathAn, "styp_counts_readDGE_TopHat.annotated.csv"),
            sep = ",", row.names = TRUE, col.names = NA, quote = FALSE) 
```

Try fixing gene names here before continuing??
# Obtain two tables, one for each species, with first column the gene id, the second column for locus_tag - will need to use this later.
To retrieve the id and locus_tad for each gene entry, run the following perl command in a terminal, then open with Word, copy to excel,
filter for those that start with gene, then remove duplicates based on the column that has the gene ids, then copy to gedit and save as txt file.      
      
perl -nle '($id)=/ID=([^;]*)/; ($prod)=/locus_tag=([^;]*)/; print "$id\t$prod"' selent.gff > salent.ids.txt      
     
Below we get the headers of transcripts from the IDs text files we just created then clean them up so that we get just the Locus tags for each gene.
```{r}
library(data.table)
# Transcript files to extract locus_tag names from for each species:
tempSalenTrans <- readDNAStringSet(salenTranscript)
tempSaltypTrans <- readDNAStringSet(saltyTranscript)

# Locus_tag gene IDs text files:
sentIDs <- fread(paste(projRefPath, "salent.ids.txt", sep = ""), header = FALSE)
stypIDs <- fread(paste(projRefPath, "saltyp.ids.txt", sep = ""), header = FALSE)

# Take a look at the naming structures of the transcripts to determine how to retrieve locus_tags:
names(tempSalenTrans)[1:5]
names(tempSaltypTrans)[1:5] # This one is different from the Salent species above.

# Extract the locus_tag names from the transcript headers for each species:
# For salent:
namesSalenTrans <- sub("] \\[db_xref.*$", "", names(tempSalenTrans))
namesSalenTrans[1] # note how locus_tag is at the end now.
namesSalenTrans <- sub("^.*locus_tag=", "", namesSalenTrans)
namesSalenTrans[1] # Now it's fixed, but there are others formatted differently:
namesSalenTrans[265] # Note the format of this entry
namesSalenTrans <- sub("\\].*$", "", namesSalenTrans)
namesSalenTrans[265:365]

# For saltyp:
namesSaltypTrans <- sub("] \\[db_xref.*$", "", names(tempSaltypTrans))
namesSaltypTrans[1] # Note how locus_tag is in the middle in this one
namesSaltypTrans <- sub("^.*locus_tag=", "", namesSaltypTrans)
namesSaltypTrans[1] # Now the pattern before the locus_tag is removed
namesSaltypTrans <- sub("\\].*$", "", namesSaltypTrans)
namesSaltypTrans[1] # Only the locus_tag remains
namesSaltypTrans[265:365]
```
Now we need to create a table that has a column for the gene names and one column for the matching locus tag:
```{r}

```


```{r}
library("Biostrings")
library("edgeR")
library("reshape2")
# Rows common to both Strain count tables, and needing to be removed:
rowsRemove <- c("__alignment_not_unique",
                "__ambiguous",
                "__no_feature",
                "__not_aligned",
                "__too_low_aQual")

# For salent:
#Check that the rows specified from the last rows of the table include all and only the RNA genes:
row.names(countsSent)[4325:4435]
rowsRemoveSent <- row.names(countsSent)[4325:4435] # These are unwanted ribosomal genes 
countsSent <- countsSent[-which(rownames(countsSent) %in% c(rowsRemoveSent, rowsRemove)), ]

# For saltyp:
row.names(countsStyp)[4554:4676]
rowsRemoveStyp <- row.names(countsStyp)[4554:4676] # These are unwanted ribosomal genes 
countsStyp <- countsStyp[-which(rownames(countsStyp) %in% c(rowsRemoveStyp, rowsRemove)), ]

# Filter weakly-expressed features without at least 1 read per million in the n of the samples, where n is the size of the smallest group of replicates (here, n = 1 for the single knockdown - 8 - group)
cpmsSent <- cpm(countsSent)  # counts per million
cpmsStyp <- cpm(countsStyp)  # counts per million

keep <- rowSums(cpmsSent >1) >= 1
countsSent  <- countsSent[keep,]
keep <- rowSums(cpmsStyp >1) >= 1
countsStyp  <- countsStyp[keep,]

# Visualize and inspect the count table:
head( countsSent[,order(salentMetaDt$Condition)], 5)
head( countsStyp[,order(saltypMetaDt$Condition)], 5)
```
```{r}
# Create a DGEList object (edgeR's container for RNA-seq count data):
dSent <- DGEList(counts = countsSent, group = salentMetaDt$Condition)
dStyp <- DGEList(counts = countsStyp, group = saltypMetaDt$Condition)

# Estimate normalization factors using:
dSent <- calcNormFactors(dSent)
dStyp <- calcNormFactors(dStyp)
```
Inspect the relationships between samples using a multidimensional scaling (MDS) plot:
```{r}
# For S. enteriditis:
plotMDS(dSent, labels = salentMetaDt$Condition, 
        col = rainbow(length(levels(factor(salentMetaDt$Condition))))[factor(salentMetaDt$Condition)], 
        cex = 1, main = "Salmonella enteritidis MDS") 
```

```{r}
# For S. typhimurium:
plotMDS(dStyp, labels = saltypMetaDt$Condition, 
        col = rainbow(length(levels(factor(saltypMetaDt$Condition))))[factor(saltypMetaDt$Condition)], 
        cex = 1, main = "Salmonella typhimurium MDS") 
```

Print the MDS files to a fresh results directory:
```{r}
# Create the results directories for each species:
resultsSent <- "sent_28-May-2019-TopHat-edgeR"
dir.create(paste(sharedPathAn, resultsSent, sep = ""), showWarnings = TRUE, recursive = FALSE)

resultsStyp <- "styp_28-May-2019-TopHat-edgeR"
dir.create(paste(sharedPathAn, resultsStyp, sep = ""), showWarnings = TRUE, recursive = FALSE)

# Generate the MDS plots:
pdf(file.path(paste(sharedPathAn, resultsSent, sep = ""), "sent_MDS-edgeR_TopHat.pdf")) 
plotMDS(dSent, labels = salentMetaDt$Condition, 
        col = rainbow(length(levels(factor(salentMetaDt$Condition))))[factor(salentMetaDt$Condition)], 
        cex = 1, main = "Salmonella enteritidis MDS") 
dev.off()

pdf(file.path(paste(sharedPathAn, resultsStyp, sep = ""), "styp_MDS-edgeR_TopHat.pdf"))
plotMDS(dStyp, labels = saltypMetaDt$Condition, 
        col = rainbow(length(levels(factor(saltypMetaDt$Condition))))[factor(saltypMetaDt$Condition)], 
        cex = 1, main = "Salmonella typhimurium MDS") 
dev.off()
```
Estimate tagwise dispersion (simple design):
```{r}
dSent <- estimateCommonDisp(dSent)
dSent <- estimateTagwiseDisp(dSent)
dStyp <- estimateCommonDisp(dStyp)
dStyp <- estimateTagwiseDisp(dStyp)
```

Create a visual representation of the mean-variance relationship using the plotMeanVar and plotBCV function:
```{r}
# For S. enteriditis
plotMeanVar(dSent, show.tagwise.vars = TRUE, NBline = TRUE)
plotBCV(dSent)
```
```{r}
# For S. typhimurium
plotMeanVar(dStyp, show.tagwise.vars = TRUE, NBline = TRUE)
plotBCV(dStyp)
```
Print to file the plotMeanVar and BCV plots:
```{r}
# For S. enteriditis
pdf(file.path(paste(sharedPathAn, resultsSent, sep = ""), "sent_plotMeanVar-edgeR.pdf")) 
plotMeanVar(dSent, show.tagwise.vars = TRUE, NBline = TRUE)
dev.off()
pdf(file.path(paste(sharedPathAn, resultsSent, sep = ""), "sent_plotBCV-edgeR.pdf")) 
plotBCV(dSent)
dev.off()

# For S. typhimurium
pdf(file.path(paste(sharedPathAn, resultsStyp, sep = ""), "styp_plotMeanVar-edgeR.pdf")) 
plotMeanVar(dStyp, show.tagwise.vars = TRUE, NBline = TRUE)
dev.off()
pdf(file.path(paste(sharedPathAn, resultsStyp, sep = ""), "styp_plotBCV-edgeR.pdf")) 
plotBCV(dStyp)
dev.off()
```
Test for differential expression for complex designs - Here our designs are the same for both species, so no need to create two separate design matrices:
```{r}
library(edgeR)
design <- model.matrix(object = ~Condition, salentMetaDt)
design
```
Estimate pairwise dispersion values, relative to the design matrix, using the Cox-Reid (CR)-adjusted likelihood:
```{r}
# For S. enteriditis:
d2Sent <- estimateGLMTrendedDisp(dSent, design)
d2Sent <- estimateGLMTagwiseDisp(d2Sent, design)

# For S. typhimurium:
d2Styp <- estimateGLMTrendedDisp(dStyp, design)
d2Styp <- estimateGLMTagwiseDisp(d2Styp, design)
```

Given the design matrix and dispersion estimates, fit a GLM to each feature:
```{r}
fSent <- glmFit(d2Sent, design)
fStyp <- glmFit(d2Styp, design)
```

Perform a likelihood ratio test specifying the difference of interest (here, knockouts vesus control, which corresponds to the WHICH column in the design matrix)
```{r}
deSent <- glmLRT(fSent, coef = 2:4)
deStyp <- glmLRT(fStyp, coef = 2:4)
```

Use the topTags function to present a tabular summary of the differential expression statistics:
```{r}
ttSent <- topTags(deSent, n=nrow(dSent))
head(ttSent$table)

ttStyp <- topTags(deStyp, n=nrow(dStyp))
head(ttStyp$table)
```
Inspect the depth-adjusted reads per million for some of the top differentially expressed genes:
```{r}
ncSent <- cpm(dSent, normalized.lib.sizes = TRUE)
ncStyp <- cpm(dStyp, normalized.lib.sizes = TRUE)

rnSent <- rownames(ttSent$table)
rnStyp <- rownames(ttStyp$table)

head(ncSent[rnSent, order(salentMetaDt$Condition)],5)
head(ncStyp[rnStyp, order(saltypMetaDt$Condition)],5)
```
Create a graphical summary, such as an M (log-fold change) versus A (log-average expression) plot, here showing the genes selected as differentially expressed (with a 5% false discovery rate):
```{r}
degSent <- rnSent[ttSent$table$FDR < .05]
degStyp <- rnStyp[ttStyp$table$FDR < .05]

plotSmear(dSent, de.tags = degSent)
plotSmear(dStyp, de.tags = degStyp)
```
Save the results tables:
```{r}
write.csv(ttSent$table, file = paste(sharedPathAn, resultsSent, "/sent_topTags_edgeR.csv", sep = ""))
write.csv(ttStyp$table, file = paste(sharedPathAn, resultsStyp, "/styp_topTags_edgeR.csv", sep = ""))
```

#####################################################################
############### Old                                  ################
#####################################################################
Only for this set of data, extract control columns based on their names.
```{r}
# List the controls that you want to drop from count analyses, if there are none do: c("","") 
# controlToDrop <- c("","") 
# tempCountsTopHat <- data.frame(countsTopHat)
# colnames(tempCountsTopHat) <- colnames(countsTopHat)
# countsTopHatDf   <- tempCountsTopHat[, !(colnames(tempCountsTopHat) %in% controlToDrop)]

controlToDrop <- c("","")
tempCountsTopHatSent <- data.frame(countsTopHatSent)
tempCountsTopHatStyp <- data.frame(countsTopHatStyp)
colnames(tempCountsTopHatSent) <- colnames(countsTopHatSent)
colnames(tempCountsTopHatStyp) <- colnames(countsTopHatStyp)
countsTopHatDfSent <- tempCountsTopHatSent[, !(colnames(tempCountsTopHatSent) %in% controlToDrop)]
countsTopHatDfStyp <- tempCountsTopHatStyp[, !(colnames(tempCountsTopHatStyp) %in% controlToDrop)]
```

The trick was from here
http://stackoverflow.com/questions/19321053/sum-together-columns-of-data-frame-based-on-name-type
Make the names of the columns with different libraries of the same experimental unit identical - some 
columns have the same name. Then a row sum for each unique name is done. When a library is sequenced 
twice, the numbers will be added up.

*** Note - we replicates to include for this project, so the folllowing is just a way to go with the script made with Andre.
```{r}
library("reshape2")
# For Salent:
salentMetaDtOld <- salentMetaDt
salentMetaDt$ExpUnit <- sub("-DIP_.*", "", salentMetaDt$FilenamePrefix)

countsTopHatDfSent$gene <- rownames(countsTopHatDfSent)
countsTopHatMeltSent <- melt(countsTopHatDfSent)

colnames(countsTopHatMeltSent)[2] <- "FilenamePrefix"

# For paired-end not merged:
dfSent <- merge(countsTopHatMeltSent, salentMetaDt[,c("FilenamePrefix", "ExpUnit", "Replicate")], 
            by = "FilenamePrefix", all.x = TRUE)

countsTopHatSumSent <- dcast(data = dfSent[,c("gene", "value", "ExpUnit", "Replicate", "FilenamePrefix")], 
                         gene ~ ExpUnit, sum, value.var = "value")

rownames(countsTopHatSumSent) <- countsTopHatSumSent$gene
countsTopHatSumSent$gene      <- NULL

# Now for Saltyp:
saltypMetaDtOld <- saltypMetaDt
saltypMetaDt$ExpUnit <- sub("-DIP_.*", "", saltypMetaDt$FilenamePrefix)

countsTopHatDfStyp$gene <- rownames(countsTopHatDfStyp)
countsTopHatMeltStyp <- melt(countsTopHatDfStyp)
colnames(countsTopHatMeltStyp)[2] <- "FilenamePrefix"

# For paired-end not merged:
dfStyp <- merge(countsTopHatMeltStyp, saltypMetaDt[,c("FilenamePrefix", "ExpUnit", "Replicate")], 
            by = "FilenamePrefix", all.x = TRUE)

countsTopHatSumStyp <- dcast(data = dfStyp[,c("gene", "value", "ExpUnit", "Replicate", "FilenamePrefix")], 
                         gene ~ ExpUnit, sum, value.var = "value")
rownames(countsTopHatSumStyp) <- countsTopHatSumStyp$gene
countsTopHatSumStyp$gene      <- NULL
```

Normalization after removing rDNA data.  Not sure if this is the best approach.
```{r}
library("Biostrings")
library("edgeR")
library("reshape2")
# For salent:
#Check that the rows specified from the last rows of the table include all and only the RNA genes:
row.names(countsTopHatSumSent)[4330:4435]

rowsRemove1Sent <- row.names(countsTopHatSumSent)[4330:4435] # These are unwanted ribosomal genes 
rowsRemove2Sent  <- c("__alignment_not_unique",
                      "__ambiguous",
                      "__no_feature",
                      "__not_aligned",
                      "__too_low_aQual")

countsTopHatGenesSent <- countsTopHatSumSent[-which(rownames(countsTopHatSumSent) 
                                                    %in% c(rowsRemove1Sent, rowsRemove2Sent)), ]
# To see the proportion of reads not ribosomal compared to ribosomal and all else (for fun):
countsTopHatDfSent$gene <- NULL
mean(colSums(countsTopHatGenesSent)/colSums(countsTopHatSumSent))
# [1] 0.6730904
cpmsSentTopHatGenes <- cpm(countsTopHatGenesSent)  # counts per million

# For saltyp:
row.names(countsTopHatSumStyp)[4559:4676]
rowsRemove1Styp <- row.names(countsTopHatSumStyp)[4559:4676] # These are unwanted ribosomal genes 
rowsRemove2Styp  <- c("__alignment_not_unique",
                      "__ambiguous",
                      "__no_feature",
                      "__not_aligned",
                      "__too_low_aQual")

countsTopHatGenesStyp <- countsTopHatSumStyp[-which(rownames(countsTopHatSumStyp) 
                                                    %in% c(rowsRemove1Styp, rowsRemove2Styp)), ]

# To see the proportion of reads not ribosomal compared to ribosomal and all else (for fun):
countsTopHatDfStyp$gene <- NULL
mean(colSums(countsTopHatGenesStyp)/colSums(countsTopHatSumStyp))
# [1] 0.6945043
cpmsStypTopHatGenes <- cpm(countsTopHatGenesStyp)  # counts per million
```
# Obtain two tables, one for each species, with first column the gene id, the second column for locus_tag - will need to use this later.
To retrieve the id and locus_tad for each gene entry, run the following perl command in a terminal, then open with Word, copy to excel,
filter for those that start with gene, then remove duplicates based on the column that has the gene ids, then copy to gedit and save as txt file.

perl -nle '($id)=/ID=([^;]*)/; ($prod)=/locus_tag=([^;]*)/; print "$id\t$prod"' selent.gff > salent.ids.txt

```{r}
library(data.table)
# Transcript files to extract locus_tag names from for each species:
tempSalenTrans <- readDNAStringSet(salenTranscript)
tempSaltypTrans <- readDNAStringSet(saltyTranscript)

# Locus_tag gene IDs text files:
sentIDs <- fread(paste(projRefPath, "salent.ids.txt", sep = ""), header = FALSE)
stypIDs <- fread(paste(projRefPath, "saltyp.ids.txt", sep = ""), header = FALSE)

# Take a look at the naming structures of the transcripts to determine how to retrieve locus_tags:
names(tempSalenTrans)[1:5]
names(tempSaltypTrans)[1:5] # This one is different from the Salent species above.

# Extract the locus_tag names from the transcript headers for each species:
# For salent:
namesSalenTrans <- sub("] \\[db_xref.*$", "", names(tempSalenTrans))
namesSalenTrans[1] # note how locus_tag is at the end now.
namesSalenTrans <- sub("^.*locus_tag=", "", namesSalenTrans)
namesSalenTrans[1] # Now it's fixed, but there are others formatted differently:
namesSalenTrans[265] # Note the format of this entry
namesSalenTrans <- sub("\\].*$", "", namesSalenTrans)
namesSalenTrans[265:365]

# For saltyp:
namesSaltypTrans <- sub("] \\[db_xref.*$", "", names(tempSaltypTrans))
namesSaltypTrans[1] # Note how locus_tag is in the middle in this one
namesSaltypTrans <- sub("^.*locus_tag=", "", namesSaltypTrans)
namesSaltypTrans[1] # Now the pattern before the locus_tag is removed
namesSaltypTrans <- sub("\\].*$", "", namesSaltypTrans)
namesSaltypTrans[1] # Only the locus_tag remains
namesSaltypTrans[265:365]
```
Switch locus_tags with matching gene ids:
```{r}
# Sent:
namesSalenList <- as.data.table(namesSalenTrans)
namesSaltypList <- as.data.table(namesSaltypTrans)
```

Take a look:
```{r}
sentIDs[1:2]
```

```{r}
namesSalenList[1:2]
```

Take a look:
```{r}
stypIDs[1:2]
```

```{r}
namesSaltypList[1:2]
```

```{r}
library(data.table)
# For Sent:
setDT(namesSalenList)
setDT(sentIDs)
sapply(names(namesSalenList), #(or to whichever are the relevant columns)
       function(cc) namesSalenList[sentIDs, (cc) := #merge, replace
                                     #need to pass a _named_ vector to 'on', so use setNames
                                     i.V1, on = setNames("V2", cc)])
namesTranssent <- as.list(namesSalenList$namesSalenTrans)
# For Styp:
setDT(namesSaltypList)
setDT(stypIDs)
sapply(names(namesSaltypList), #(or to whichever are the relevant columns)
       function(cc) namesSaltypList[stypIDs, (cc) := #merge, replace
                                     #need to pass a _named_ vector to 'on', so use setNames
                                     i.V1, on = setNames("V2", cc)])
namesTransstyp <- as.list(namesSaltypList$namesSaltypTrans)

# Here we are using the sent.ids.txt table to match the id to locus_tag so tie together the gene lengths, which is retrieved using the locus_tag. We haven't updated the table so that the rownames are the locus tag yet.

# Create a vector with names:
# For Sent:
geneLengthSent <- setNames(width(tempSalenTrans), namesTranssent)
geneLengthSent[1]
temp2sent <- merge(countsTopHatGenesSent, geneLengthSent, by.x = 0, by.y = 0)
temp2sent[1:3,]
rownames(temp2sent) <- temp2sent$Row.names
#temp2sent$Row.names <- NULL
temp2sent[1:3,]

# For Styp:
geneLengthStyp <- setNames(width(tempSaltypTrans), namesTransstyp)
geneLengthStyp[1]
temp2styp <- merge(countsTopHatGenesStyp, geneLengthStyp, by.x = 0, by.y = 0)
temp2styp[1:3,]
rownames(temp2styp) <- temp2styp$Row.names
#temp2styp$Row.names <- NULL
temp2styp[1:3,]

# Here I'm getting the gene IDs replaced with the locus_tags.
# For Sent:
new <- as.data.table(temp2sent)
new$locus_tag <- new$Row.names
names(sentIDs)[1] <- "SppAbbr"
names(sentIDs)[2] <- "newValue"
setkey(new, locus_tag)
setkey(sentIDs, SppAbbr)
new[sentIDs, locus_tag := newValue]

new[!duplicated(new$locus_tag),]
new$Row.names <- NULL
new2 <- as.data.frame(new)
rownames(new2) <- new2$locus_tag
new2$Row.names <- NULL
new2$locus_tag <- NULL
temp2sent <- new2
write.table(temp2sent, file = file.path(paste(sharedPathAn, sep = ""), "Sent_readDGEcountsWithGeneLength.csv"),
            append = FALSE, sep = ",", col.names = NA)

# For Styp:
new <- as.data.table(temp2styp)
new$locus_tag <- new$Row.names
names(stypIDs)[1] <- "SppAbbr"
names(stypIDs)[2] <- "newValue"
setkey(new, locus_tag)
setkey(stypIDs, SppAbbr)
new[stypIDs, locus_tag := newValue]

new[!duplicated(new$locus_tag),]
new$Row.names <- NULL
new2 <- as.data.frame(new)
rownames(new2) <- new2$locus_tag
new2$Row.names <- NULL
new2$locus_tag <- NULL
temp2styp <- new2
write.table(temp2styp, file = file.path(paste(sharedPathAn, sep = ""), "Styp_readDGEcountsWithGeneLength.csv"),
            append = FALSE, sep = ",", col.names = NA)
```
rpkm edgeR Salent rna-seq:
```{r}
summary(temp2sent)
library("edgeR")
salent_rpkmCount <- rpkm(temp2sent[,1:4], # All columns except y which is col 5
                         gene.length = temp2sent$y, normalized.lib.sizes = TRUE)
salent_rpkmCount[1:5,]
```

rpkm edgeR Salty rna-seq:
```{r}
summary(temp2styp)
library("edgeR")
saltyp_rpkmCount <- rpkm(temp2styp[,1:4], # All columns except y which is col 5
                         gene.length = temp2styp$y, normalized.lib.sizes = TRUE)
saltyp_rpkmCount[1:5,]
```

Setting up for Differential Analysis: 

Counts per million (cpm) from the gene counts from TopHat: 
```{r}
library("reshape2")
# For both Sent and Styp:
metadataColsToMerge <- c("LibraryName", "FilenamePrefix", "Condition", "Replicate", "ExpUnit") 
condition <- c("ko342", "ko42", "ko8", "control")

# For Sent:
countsRawSent <- salent_rpkmCount[,1:4] # instead of temp2Sent
cpmsSent <- cpm(countsRawSent) # counts per million
countsForBarPlotsSent <- melt(cpmsSent, id.vars = colnames(cpmsSent))
colnames(countsForBarPlotsSent)[1] <- "SequenceID"
colnames(countsForBarPlotsSent)[2] <- "ExpUnit"
colnames(countsForBarPlotsSent)[3] <- "Read_Counts"
countsSent <- merge(countsForBarPlotsSent, unique(metadataRawPairs[,c(metadataColsToMerge)]),  
                    by = "ExpUnit", all.x = TRUE)
countsSent$Condition <- factor(countsSent$Condition, levels = c(condition))
length(unique(countsSent$Condition))
countsSent[1:5,]

# For Styp:
countsRawStyp <- saltyp_rpkmCount[,1:4] # instead of temp2Styp
cpmsStyp <- cpm(countsRawStyp) # counts per million
countsForBarPlotsStyp <- melt(cpmsStyp, id.vars = colnames(cpmsStyp))
colnames(countsForBarPlotsStyp)[1] <- "SequenceID"
colnames(countsForBarPlotsStyp)[2] <- "ExpUnit"
colnames(countsForBarPlotsStyp)[3] <- "Read_Counts"
countsStyp <- merge(countsForBarPlotsStyp, unique(metadataRawPairs[,c(metadataColsToMerge)]),  
                    by = "ExpUnit", all.x = TRUE)
countsStyp$Condition <- factor(countsStyp$Condition, levels = c(condition))
length(unique(countsStyp$Condition))
countsStyp[1:5,]
```
Finding potential house-keeping (HK) genes: 
```{r}
# For Sent:
colnames(salent_rpkmCount)
logRpkmCountSent      <- data.frame(log10(salent_rpkmCount[,1:4]+1))
logRpkmCountSent$mean <- apply(logRpkmCountSent, 1, mean)
logRpkmCountSent$min  <- apply(logRpkmCountSent, 1, min)
logRpkmCountSent$max  <- apply(logRpkmCountSent, 1, max)
logRpkmCountSent$SD   <- apply(logRpkmCountSent, 1, sd)

potentialHKSent <- subset(logRpkmCountSent, min > 0 & SD < 0.25)
potentialHKSent <- potentialHKSent[order(-potentialHKSent$mean),]  

write.table(potentialHKSent, 
            file   = file.path(paste(sharedPathAn, sep = ""), "sent_potential_houseKeepingGenes.csv"),
            append = FALSE, sep = ",", col.names = NA)

# For Styp:
colnames(saltyp_rpkmCount)
logRpkmCountStyp      <- data.frame(log10(saltyp_rpkmCount[,1:4]+1))
logRpkmCountStyp$mean <- apply(logRpkmCountStyp, 1, mean)
logRpkmCountStyp$min  <- apply(logRpkmCountStyp, 1, min)
logRpkmCountStyp$max  <- apply(logRpkmCountStyp, 1, max)
logRpkmCountStyp$SD   <- apply(logRpkmCountStyp, 1, sd)

potentialHKStyp <- subset(logRpkmCountStyp, min > 0 & SD < 0.25)
potentialHKStyp <- potentialHKStyp[order(-potentialHKStyp$mean),]  

write.table(potentialHKStyp, 
            file   = file.path(paste(sharedPathAn, sep = ""), "styp_potential_houseKeepingGenes.csv"),
            append = FALSE, sep = ",", col.names = NA)
```


Generating the samples table for each species:
```{r}
# For Sent:
colnames(salent_rpkmCount)
samplesSent <- data.frame(colnames(salent_rpkmCount))
colnames(samplesSent)[1] <- "ExpUnit"
samplesSent$Condition <- samplesSent$ExpUnit
samplesSent$Condition <- sub("^3346-", "", samplesSent$Condition, ignore.case = FALSE)
samplesSent$Condition <- sub("_.*$", "", samplesSent$Condition, ignore.case = FALSE)
samplesSent$Condition <- sub("WT", "Control", samplesSent$Condition, ignore.case = FALSE)
samplesSent$Treatment <- samplesSent$Condition
samplesSent$Treatment <- sub("^42", "ko42", samplesSent$Treatment)
samplesSent$Treatment <- sub("^342", "ko342", samplesSent$Treatment)
samplesSent$Treatment <- sub("^8", "ko8", samplesSent$Treatment)

# Filter low count genes from Sent:
raw_countsSent <- temp2sent[,1:4]
# Need to only filter presence by no more than the number of actual libraries (ExpUnit):
filterSent <- apply(raw_countsSent, 1, function(x) length(x[x>5]) >=3) # Filtered those at least expressed in 3 of the 4 libraries
filteredSent <- raw_countsSent[filterSent,]
head(filteredSent[,order(samplesSent$Condition)],5)
```

```{r}
# For Styp:
colnames(saltyp_rpkmCount)
samplesStyp <- data.frame(colnames(salent_rpkmCount))
colnames(samplesStyp)[1] <- "ExpUnit"
samplesStyp$Condition <- samplesStyp$ExpUnit
samplesStyp$Condition <- sub("^3346-", "", samplesStyp$Condition, ignore.case = FALSE)
samplesStyp$Condition <- sub("_.*$", "", samplesStyp$Condition, ignore.case = FALSE)
samplesStyp$Condition <- sub("WT", "Control", samplesStyp$Condition, ignore.case = FALSE)
samplesStyp$Treatment <- samplesStyp$Condition
samplesStyp$Treatment <- sub("^42", "ko42", samplesStyp$Treatment)
samplesStyp$Treatment <- sub("^342", "ko342", samplesStyp$Treatment)
samplesStyp$Treatment <- sub("^8", "ko8", samplesStyp$Treatment)
# Filter low count genes from Styp:
raw_countsStyp <- temp2styp[,1:4]
# Need to only filter presence by no more than the number of actual libraries (ExpUnit):
filterStyp <- apply(raw_countsStyp, 1, function(x) length(x[x>5]) >=3) # Filtered those at least expressed in 3 of the 4 libraries
filteredStyp <- raw_countsStyp[filterStyp,]
head(filteredStyp[,order(samplesStyp$Condition)],5)
```
```{r}
library(DESeq2)
library(edgeR)
# For Sent:
#Create a DGEList object (edgeR's container for RNA-seq count data): 
#dTopHSent = DGEList(counts=raw_countsSent, group=samplesSent$Condition) 
dTopHSent = DGEList(counts=filteredSent, group=samplesSent$Condition) 
#Estimate normalization factors using, RNA composition and adjust for read depth: 
dTopHSent <- calcNormFactors(dTopHSent)

# For Styp:
#Create a DGEList object (edgeR's container for RNA-seq count data): 
#dTopHStyp = DGEList(counts=raw_countsStyp, group=samplesStyp$Condition)
dTopHStyp = DGEList(counts=filteredStyp, group=samplesStyp$Condition) 
#Estimate normalization factors using, RNA composition and adjust for read depth: 
dTopHStyp <- calcNormFactors(dTopHStyp)
```

Inspect the relationships between samples using a multidimensional scaling (MDS) plot, as shown in Figure 4:     
For Sent:
```{r}
results <- "sent_27-May-2019-TopHat-edgeR"
dir.create(paste(sharedPathAn, results, sep = ""), showWarnings = TRUE, recursive = FALSE)
pdf(file.path(paste(sharedPathAn, results, sep = ""), "sent_MDS-edgeR_TopHat.pdf")) 
plotMDS(dTopHSent, labels = samplesSent$Treatment, 
        col = rainbow(length(levels(factor(samplesSent$Treatment))))[factor(samplesSent$Treatment)], cex = 1, main = "Salmonella enteritidis MDS") 
dev.off()
```


For Styp:
```{r}
resultsStyp <- "styp_27-May-2019-TopHat-edgeR"
dir.create(paste(sharedPathAn, resultsStyp, sep = ""), showWarnings = TRUE, recursive = FALSE)
pdf(file.path(paste(sharedPathAn, resultsStyp, sep = ""), "styp_MDS-edgeR_TopHat.pdf")) 
plotMDS(dTopHStyp, labels = samplesStyp$Treatment, 
        col = rainbow(length(levels(factor(samplesStyp$Treatment))))[factor(samplesStyp$Treatment)], cex = 1, main = "Salmonella typhimurium MDS") 
dev.off()
```
# ***Over here!!! Need to set this up so that the replicates show - as it is, they are all lumped together per ExpUnit and this needs replicates data specifics:
```{r}
# Estimate tagwise dispersion (simple design) using:
dTopHSent <- estimateCommonDisp(dTopHSent)
dTopHSent <- estimateTagwiseDisp(dTopHSent)

# Create a visual representation of the mean-variance relationship using the plotMeanVar and plotBCV functions, as follows:
pdf(file.path(paste(sharedPathAn, results, sep = ""), "sent_27-May-2019-TopHat-edgeR_plotMeanVar.pdf")) 
plotMeanVar(dTopHSent, show.tagwise.vars = TRUE, NBline = TRUE)
dev.off()

pdf(file.path(paste(sharedPathAn, results, sep = ""), "sent_27-Jan-2019-TopHat-edgeR_plotBCV.pdf")) 
plotBCV(dTopHSent)
dev.off()
```

# Test for differential expression ('classic' edgeR), as follows:
```{r}
#Create a DGEList object (edgeR's container for RNA-seq count data): 
dTopHSent = DGEList(counts=filteredSent, group=samplesSent$Treatment) 
#Estimate normalization factors using, RNA composition and adjust for read depth: 
dTopHSent <- calcNormFactors(dTopHSent)
dTopHSent <- estimateCommonDisp(dTopHSent)

# error here: Error in .compressDispersions(y, dispersion) : 
# dispersions must be finite non-negative values
dTopHSent <- estimateTagwiseDisp(dTopHSent)

deTopHSent <- exactTest(dTopHSent, pair = c("fe", "noFe"))
toptags_fe_noFe <- topTags(deTopH)
design <- model.matrix(~group, data=dTopH$samples)
fit <- glmQLFit(dTopH, design)
qlf <- glmQLFTest(fit, coef=1:2)
topTags(qlf)

design2 <- model.matrix(~0+group,data=dTopH$samples)
fit2 <- glmQLFit(dTopH, design2)
qlf2 <- glmQLFTest(fit2, coef=1:2)
topTags(qlf2)

# 3.2.6 ANOVA-like test for any differences - will find any genes that differ between any of the treatment conditions - whether contrasts are non-zero.
dTopHSent = DGEList(counts=filteredSent, group = samplesSent$Condition) 
#Estimate normalization factors using, RNA composition and adjust for read depth: 
dTopHSent <- calcNormFactors(dTopHSent)
dTopHSent <- estimateCommonDisp(dTopHSent)
dTopHSent <- estimateTagwiseDisp(dTopHSent)
deTopHSent <- exactTest(dTopHSent)
toptags_conditionsSent <- topTags(deTopHSent)
design3 <- model.matrix(~0+group,data=dTopH$samples)
fit3 <- glmQLFit(dTopH, design3)
qlf3 <- glmQLFTest(fit3, coef=1:4)
topTags(qlf3)


Group <- factor(paste(samples$Treatment, samples$Condition, sep = "."))
cbind(samples, Group=Group)
design4 <- model.matrix(~0+Group)
colnames(design4) <- levels(Group)
fit4 <- glmQLFit(dTopH, design4)

my.contrasts <- makeContrasts(
  fe_KO_342_vs_fe_WT = fe.342-fe.Control,
  fe_KO_42_vs_fe_WT = fe.42-fe.Control,
  fe_KO_8_vs_fe_WT = fe.8-fe.Control,
  noFe_KO_342_vs_noFe_WT = noFe.342-noFe.Control,
  noFe_KO_42_vs_noFe_WT = noFe.42-noFe.Control,
  noFe_KO_8_vs_noFe_WT = noFe.8-noFe.Control,
  fe_WT_vs_noFe_WT = fe.Control-noFe.Control,
  levels = design4
)

qlf <- glmQLFTest(fit4, contrast = my.contrasts[,"fe_KO_342_vs_fe_WT"])
qlf <- glmQLFTest(fit4, contrast = my.contrasts[,"fe_WT_vs_noFe_WT"])
```

```{r}
# edgeR - using glm 
#Create a design matrix to specify the factors that are expected to affect expression levels: 
designTopH = model.matrix( ~ Group.1, dat_agg2[c(10:15),]) ## samples is your sample sheet, treatment is a column in the sample sheet 
designTopH 

### Here it is pH6 - pH2.5 (so positive fold change indicates higher expression in Normal, negative is higher in Affected) 
#Estimate dispersion values, relative to the design matrix, using the Cox-Reid (CR)-adjusted likelihood 
d2TopH = estimateGLMCommonDisp(dTopH, designTopH) 
d2TopH = estimateGLMTrendedDisp(d2TopH, designTopH) 
d2TopH = estimateGLMTagwiseDisp(d2TopH, designTopH) 

#plot the mean-variance relationship: 
pdf(file.path(paste(sharedPathAn,results,sep=""),"mean.variance-edgeR_TopHat.pdf")) 

# Plot the relationship between mean expression and variance of expression 
plotMeanVar(d2TopH, show.tagwise.vars=TRUE, NBline=TRUE,main="MeanVar") 

# Plot the Biological Coefficient of Variation (as opposed to technical coefficient of variation) 
plotBCV(d2TopH,main="BCV") 
dev.off() 

#Given the design matrix and dispersion estimates, fit a GLM to each feature: 
fTopH = glmFit(d2TopH, designTopH) 

#Perform a likelihood ratio test, specifying the difference of interest 
deTopH = glmLRT(fTopH, coef=2) ## Treatment coefficient 

#Use the topTags function to present a tabular summary of the differential expression statistics 
ttTopH = topTags(deTopH, n=nrow(dTopH)) ## all tags, sorted 
head(ttTopH$table) ## Check result 
table(ttTopH$table$FDR< 0.05) ## the number of "Statistically Differentially Expressed Genes" at an FDR of 0.05 

#Inspect the depth-adjusted reads per million for some of the top differentially expressed genes: 
ncTopH = cpm(dTopH, normalized.lib.sizes=TRUE) 
rnTopH = rownames(ttTopH$table) 
head(ncTopH[rnTopH,order(dat_agg2$Group.1[10:15])],5) 

#Plot the M (log-fold change) versus A (log-average expression) 
degTopH = rnTopH[ttTopH$table$FDR < .05] 
pdf(file.path(paste(sharedPathAn,results,sep=""),"smear-edgeR_TopHat-T3.pdf")) 
plotSmear(dTopH, de.tags=degTopH,main="Smear") 
dev.off() 
 
#Save the result table as a CSV file: 
write.table(ttTopH$table,file=file.path(paste(sharedPathAn,results,sep=""),"toptags_edgeR_TopHat.annotated-T3.csv"), append = FALSE, sep =",", col.names=NA)                      
```

**** Over here - Andre, can we delete the next chunk?
Prepare data for bar plot
```{r}
cazyCounts$gene <- rownames(cazyCounts)
cazyDataMelt <- melt(cazyCounts)

cazyCounts <- merge(cpms, cazyData[,c(1,3,5,6,7,8,9,20)],  by.x = 0, by.y = "SequenceID", all = FALSE, all.y = TRUE)

#countsTopHatMelt <- countsTopHatMelt[countsTopHatMelt$value>0,]
colnames(countsTopHatMelt)[2] <- "LibraryName"
df <- merge(countsTopHatMelt, metadataProcessed[,c(1,8)], by= "LibraryName", all.x=TRUE)

countsTopHatSum <- dcast(data = df[,2:4], gene ~ ExpUnit, sum, value.var="value")

cazyCountsT <- t(cazyCounts)
```
